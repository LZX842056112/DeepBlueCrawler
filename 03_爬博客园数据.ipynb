{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a946c8c9-bec2-4c24-bd2b-76b03a3b5ec4",
   "metadata": {},
   "source": [
    "# 爬博客园数据\n",
    "从博客园网站爬取特定用户的博客数据，并将爬取到的信息保存到本地文件中。\n",
    "\n",
    "3 种方法只需要掌握 2 种，其中正则是必需的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758aef08-4935-443f-bcd5-8609e999319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态码：200\n",
      "网页编码：utf-8\n",
      "\n",
      "网页源代码：\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"zh-cn\">\n",
      "<head>\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
      "    <meta name=\"referrer\" content=\"origin-when-cross-origin\" />\n",
      "    \n",
      "    \n",
      "    \n",
      "    <meta http-equiv=\"Cache-Control\" content=\"no-transform\" />\n",
      "    <meta http-equiv=\"Cache-Control\" content=\"no-siteapp\" />\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n",
      "    <title>刘建平Pinard - 博客园</title>\n",
      "    <link rel=\"icon\" id=\"favicon\" href=\"https://assets.cnblogs.com/favicon_v3_2.ico\" type=\"image/x-icon\" />\n",
      "    <link rel=\"canonical\" href=\"https://www.cnblogs.com/pinard\" />\n",
      "    \n",
      "    <link rel=\"stylesheet\" href=\"/css/blog-common.min.css?v=DTx8XrQTETRXxrlh9YxMFto-XNDgQSH6APv3R0bgafA\" />\n",
      "    \n",
      "\n",
      "    <link id=\"MainCss\" rel=\"stylesheet\" href=\"/skins/blacklowkey/bundle-blacklowkey.min.css?v=QmhJuqutwsyRVPwV8wOHP8zM5sTwHe5vHM1HxnzZE-8\" />\n",
      "        \n",
      "    <link type=\"text/css\" rel=\"stylesheet\" href=\"https://www.cnblogs.com/pinard/custom.css?v=YY/B9GiizlktSBml7X95KXekn&#x2B;g=\" />\n",
      "    \n",
      "    <link id=\"mobile-style\" media=\"only screen and (max-width: 767px)\" type=\"text/css\" rel=\"stylesheet\" href=\"/skins/blacklowkey/bundle-blacklowkey-mobile.min.css?v=TGgzAnUTquN6wMXT09snwJjWexfhNEYem4MFJpmGGsE\" />\n",
      "    \n",
      "    <link type=\"application/rss+xml\" rel=\"alternate\" href=\"https://www.cnblogs.com/pinard/rss\" />\n",
      "    <link type=\"application/rsd+xml\" rel=\"EditURI\" href=\"https://www.cnblogs.com/pinard/rsd.xml\" />\n",
      "    <link type=\"application/wlwmanifest+xml\" rel=\"wlwmanifest\" href=\"https://www.cnblogs.com/pinard/wlwmanifest.xml\" />\n",
      "    \n",
      "\n",
      "    <script>\n",
      "        var currentBlogId = 311024;\n",
      "        var currentBlogApp = 'pinard';\n",
      "        var isLogined = false;\n",
      "        var isBlogOwner = false;\n",
      "        var skinName = 'BlackLowKey';\n",
      "        var visitorUserId = '';\n",
      "        var hasCustomScript = false;\n",
      "        window.cb_enable_mathjax = true;\n",
      "        window.mathEngine = 0;\n",
      "        window.codeHighlightEngine = 1;\n",
      "        window.enableCodeLineNumber = false;\n",
      "        window.codeHighlightTheme = 'cnblogs';\n",
      "        window.darkModeCodeHighlightTheme = 'vs2015';\n",
      "        window.isDarkCodeHighlightTheme = false;\n",
      "        window.isDarkModeCodeHighlightThemeDark = true;\n",
      "        window.isDisableCodeHighlighter = false;\n",
      "        window.enableCodeThemeTypeFollowSystem = false;\n",
      "        window.enableMacStyleCodeBlock = false;\n",
      "    </script>\n",
      "    <script src=\"https://assets.cnblogs.com/scripts/jquery-3.3.1.min.js\"></script>\n",
      "    <script src=\"https://cdn-www.cnblogs.com/js/blog-common.min.js?v=i40Qi5qH11J7fzF3AhkLFQXfHa7iRdJybVmUAeyd9YM\"></script>\n",
      "    \n",
      "</head>\n",
      "<body class=\"skin-blacklowkey has-navbar mathjax2 has-bannerbar\">\n",
      "    <a name=\"top\"></a>\n",
      "            <a href=\"https://developer.huawei.com/consumer/cn/activity/harmonyos-incentive/2025/?ha_source=bkyyg&amp;ha_sourceId=89000238\" onclick=\"countCreativeClicks('C0-鸿蒙激励计划')\" target=\"_blank\" rel=\"nofollow\">\n",
      "                <div class=\"imagebar forpc\" style=\"cursor: pointer; background-size: contain;background-image: url(https://img2024.cnblogs.com/blog/35695/202510/35695-20251010212447135-1804228784.jpg);\">\n",
      "                    <img src=\"https://img2024.cnblogs.com/blog/35695/202510/35695-20251010212255256-1785693606.jpg\" style=\"\" onload=\"countCreativeImpressions('C0-鸿蒙激励计划')\" />\n",
      "                    \n",
      "                    <img src=\"https://img2024.cnblogs.com/blog/35695/202510/35695-20251010212353663-2075018926.jpg\" class=\"right\" />\n",
      "                    <span id=\"c0_impression\" style=\"display:none\"></span>\n",
      "                </div>\n",
      "            </a>\n",
      "        <div id=\"imagebar\" class=\"imagebar-mobile imagebar-text-mobile formobile\">\n",
      "                <a href=\"https://qoder.com/\" onclick=\"countCreativeClicks('M2-Qoder')\" rel=\"nofollow\">\n",
      "                    <img src=\"https://img2024.cnblogs.com/blog/35695/202510/35695-20251013120031697-98414168.jpg\" alt=\"\" onload=\"countCreativeImpressionsOnMobile('M2-Qoder')\" />\n",
      "                    <span id=\"m2_impression\" style=\"display:none\"></span>\n",
      "                </a>\n",
      "        </div>\n",
      "    <div id=\"top_nav\" class=\"navbar forpc\">\n",
      "        <nav id=\"nav_main\" class=\"navbar-main\">\n",
      "            <ul id=\"nav_left\" class=\"navbar-list navbar-left\">\n",
      "                <li class=\"navbar-branding\">                    \n",
      "                    <a href=\"https://www.cnblogs.com/\" title=\"开发者的网上家园\" role=\"banner\">\n",
      "                        <img src=\"//assets.cnblogs.com/logo.svg\" alt=\"博客园logo\" />\n",
      "                    </a>\n",
      "                </li>               \n",
      "                <li><a href=\"https://cnblogs.vip/\">会员</a></li>                \n",
      "                <li><a href=\"https://www.cnblogs.com/cmt/p/18500368\">众包</a></li>\n",
      "                <li><a href=\"https://news.cnblogs.com/\" onclick=\"countClicks('nav', 'skin-navbar-news')\">新闻</a></li>\n",
      "                <li><a href=\"https://q.cnblogs.com/\" onclick=\"countClicks('nav', 'skin-navbar-q')\">博问</a></li>\n",
      "                <li><a href=\"https://ing.cnblogs.com/\" onclick=\"countClicks('nav', 'skin-navbar-ing')\">闪存</a></li>\n",
      "                <li><a href=\"https://www.cnblogs.com/cmt/p/19081960\">赞助商</a></li>                \n",
      "                <li><a href=\"https://harmonyos.cnblogs.com/\" onclick=\"countClicks('nav', 'blog-navbar-harmonyos')\">HarmonyOS</a></li>\n",
      "                <li><a href=\"https://chat2db-ai.com/\" target=\"_blank\" onclick=\"countClicks('nav', 'skin-navbar-chat2db')\">Chat2DB</a></li>\n",
      "            </ul>\n",
      "            <ul id=\"nav_right\" class=\"navbar-list navbar-right\">\n",
      "                <li>\n",
      "                    <form id=\"zzk_search\" class=\"navbar-search dropdown\" action=\"https://zzk.cnblogs.com/s\" method=\"get\" role=\"search\">\n",
      "                        <input name=\"w\" id=\"zzk_search_input\" placeholder=\"代码改变世界\" type=\"search\" tabindex=\"3\" autocomplete=\"off\" />\n",
      "                        <button id=\"zzk_search_button\" onclick=\"window.navbarSearchManager.triggerActiveOption()\">\n",
      "                            <img id=\"search_icon\" class=\"focus-hidden\" src=\"//assets.cnblogs.com/icons/search.svg\" alt=\"搜索\" />\n",
      "                            <img class=\"hidden focus-visible\" src=\"//assets.cnblogs.com/icons/enter.svg\" alt=\"搜索\" />\n",
      "                        </button>\n",
      "                        <ul id=\"navbar_search_options\" class=\"dropdown-menu quick-search-menu\">\n",
      "                            <li tabindex=\"0\" class=\"active\" onclick=\"zzkSearch(event, document.getElementById('zzk_search_input').value)\">\n",
      "                                <div class=\"keyword-wrapper\">\n",
      "                                    <img src=\"//assets.cnblogs.com/icons/search.svg\" alt=\"搜索\" />\n",
      "                                    <div class=\"keyword\"></div>\n",
      "                                </div>\n",
      "                                <span class=\"search-area\">所有博客</span>\n",
      "                            </li>\n",
      "                                    <li tabindex=\"1\" onclick=\"zzkBlogSearch(event, 'pinard', document.getElementById('zzk_search_input').value)\">\n",
      "                                        <div class=\"keyword-wrapper\">\n",
      "                                            <img src=\"//assets.cnblogs.com/icons/search.svg\" alt=\"搜索\" />\n",
      "                                            <div class=\"keyword\"></div>\n",
      "                                        </div>\n",
      "                                        <span class=\"search-area\">当前博客</span>\n",
      "                                    </li>\n",
      "                        </ul>\n",
      "                    </form>\n",
      "                </li>\n",
      "                <li id=\"navbar_login_status\" class=\"navbar-list\">\n",
      "                    <a class=\"navbar-user-info navbar-blog\" href=\"https://i.cnblogs.com/EditPosts.aspx?opt=1\" alt=\"写随笔\" title=\"写随笔\">\n",
      "                        <img id=\"new_post_icon\" class=\"navbar-icon\" src=\"//assets.cnblogs.com/icons/newpost.svg\" alt=\"写随笔\" />\n",
      "                    </a>\n",
      "                    <a id=\"navblog-myblog-icon\" class=\"navbar-user-info navbar-blog\" href=\"https://passport.cnblogs.com/GetBlogApplyStatus.aspx\" alt=\"我的博客\" title=\"我的博客\">\n",
      "                        <img id=\"myblog_icon\" class=\"navbar-icon\" src=\"//assets.cnblogs.com/icons/myblog.svg\" alt=\"我的博客\" />\n",
      "                    </a>\n",
      "                    <a class=\"navbar-user-info navbar-message navbar-icon-wrapper\" href=\"https://msg.cnblogs.com/\" alt=\"短消息\" title=\"短消息\">\n",
      "                        <img id=\"msg_icon\" class=\"navbar-icon\" src=\"//assets.cnblogs.com/icons/message.svg\" alt=\"短消息\" />\n",
      "                        <span id=\"msg_count\" style=\"display: none\"></span>\n",
      "                    </a>\n",
      "                    <a id=\"navbar_lite_mode_indicator\" data-current-page=\"blog\" style=\"display: none\" href=\"javascript:void(0)\" alt=\"简洁模式\" title=\"简洁模式启用，您在访问他人博客时会使用简洁款皮肤展示\">\n",
      "                        <img class=\"navbar-icon\" src=\"//assets.cnblogs.com/icons/lite-mode-on.svg\" alt=\"简洁模式\" />\n",
      "                    </a>\n",
      "                    <div id=\"user_info\" class=\"navbar-user-info dropdown\">\n",
      "                        <a class=\"dropdown-button\" href=\"https://home.cnblogs.com/\">\n",
      "                            <img id=\"user_icon\" class=\"navbar-avatar\" src=\"//assets.cnblogs.com/icons/avatar-default.svg\" alt=\"用户头像\" />\n",
      "                        </a>\n",
      "                        <div class=\"dropdown-menu\">\n",
      "                            <a id=\"navblog-myblog-text\" href=\"https://passport.cnblogs.com/GetBlogApplyStatus.aspx\">我的博客</a>\n",
      "                            <a href=\"https://home.cnblogs.com/\">我的园子</a>\n",
      "                            <a href=\"https://account.cnblogs.com/settings/account\">账号设置</a>\n",
      "                            <a href=\"https://vip.cnblogs.com/my\">会员中心</a>\n",
      "                            <a href=\"javascript:void(0)\" id=\"navbar_lite_mode_toggle\" title=\"简洁模式会使用简洁款皮肤显示所有博客\">\n",
      "    简洁模式 <span id=\"navbar_lite_mode_spinner\" class=\"hide\">...</span>\n",
      "</a>\n",
      "\n",
      "                            <a href=\"javascript:void(0)\" onclick=\"account.logout();\">退出登录</a>\n",
      "                        </div>\n",
      "                    </div>\n",
      "                    <a class=\"navbar-anonymous\" href=\"https://account.cnblogs.com/signup\">注册</a>\n",
      "                    <a class=\"navbar-anonymous\" href=\"javascript:void(0);\" onclick=\"account.login()\">登录</a>\n",
      "                </li>\n",
      "            </ul>\n",
      "        </nav>\n",
      "    </div>\n",
      "\n",
      "    <div id=\"page_begin_html\">\n",
      "        <a rel=\"nofollow noopener\"  class=\"git-link\" href=\"https://github.com/ljpzzz/machinelearning\"></a>\n",
      "    </div>\n",
      "\n",
      "    <div id=\"home\">\n",
      "<div id=\"header\">\n",
      "\t<div id=\"blogTitle\">\n",
      "        <a id=\"lnkBlogLogo\" href=\"https://www.cnblogs.com/pinard/\"><img id=\"blogLogo\" src=\"/skins/custom/images/logo.gif\" alt=\"返回主页\" /></a>\t\t\n",
      "\t\t\n",
      "\n",
      "<h1><a id=\"Header1_HeaderTitle\" class=\"headermaintitle HeaderMainTitle\" href=\"https://www.cnblogs.com/pinard\">刘建平Pinard</a>\n",
      "</h1>\n",
      "<h2>十五年码农，对数学统计学，数据挖掘，机器学习，大数据平台，大数据平台应用开发，大数据可视化感兴趣。</h2>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\n",
      "\t</div>\n",
      "\t<div id=\"navigator\">\n",
      "\t\t\n",
      "<ul id=\"navList\">\n",
      "<li><a id=\"blog_nav_sitehome\" class=\"menu\" href=\"https://www.cnblogs.com/\">\n",
      "博客园</a>\n",
      "</li>\n",
      "<li>\n",
      "<a id=\"blog_nav_myhome\" class=\"menu\" href=\"https://www.cnblogs.com/pinard/\">\n",
      "首页</a>\n",
      "</li>\n",
      "<li>\n",
      "\n",
      "<a id=\"blog_nav_newpost\" class=\"menu\" href=\"https://i.cnblogs.com/EditPosts.aspx?opt=1\">\n",
      "新随笔</a>\n",
      "</li>\n",
      "<li>\n",
      "<a id=\"blog_nav_contact\" class=\"menu\" href=\"https://msg.cnblogs.com/send/%E5%88%98%E5%BB%BA%E5%B9%B3Pinard\">\n",
      "联系</a></li>\n",
      "<li>\n",
      "<a id=\"blog_nav_rss\" class=\"menu\" href=\"javascript:void(0)\" data-rss=\"https://www.cnblogs.com/pinard/rss/\">\n",
      "订阅</a>\n",
      "<!--<partial name=\"./Shared/_XmlLink.cshtml\" model=\"Model\" /></li>--></li>\n",
      "<li>\n",
      "<a id=\"blog_nav_admin\" class=\"menu\" href=\"https://i.cnblogs.com/\">\n",
      "管理</a>\n",
      "</li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "\t\t<div class=\"blogStats\">\n",
      "\t\t\t<div id=\"blog_stats_place_holder\"><script>loadBlogStats();</script></div>\n",
      "\t\t</div>\n",
      "\t</div>\n",
      "</div>\n",
      "<div id=\"main\">\n",
      "\t<div id=\"mainContent\">\n",
      "\t<div class=\"forFlow\">\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "<div id=\"homepage_top_pager\" class=\"topicListFooter\">\n",
      "    \n",
      "\n",
      "<div class=\"pager\">\n",
      "    \n",
      "    \n",
      "    \n",
      "    1\n",
      "        <a href=\"https://www.cnblogs.com/pinard?page=2\">2</a>\n",
      "        <a href=\"https://www.cnblogs.com/pinard?page=3\">3</a>\n",
      "        <a href=\"https://www.cnblogs.com/pinard?page=4\">4</a>\n",
      "        <a href=\"https://www.cnblogs.com/pinard?page=5\">5</a>\n",
      "    ···\n",
      "    <a href=\"https://www.cnblogs.com/pinard?page=14\">14</a>\n",
      "    <a href=\"https://www.cnblogs.com/pinard?page=2\">下一页</a>\n",
      "</div>\n",
      "</div>\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_11114748\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/07/01\">2019年7月1日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/11114748.html\">\n",
      "    <span>\n",
      "        XGBoost类库使用小结\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_11114748\">\n",
      "摘要：        \n",
      "在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth    <a href=\"https://www.cnblogs.com/pinard/p/11114748.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-07-01 18:10\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"11114748\" class=\"post-view-count\">阅读(66952)</span>\n",
      "<span data-post-id=\"11114748\" class=\"post-comment-count\">评论(135)</span>\n",
      "<span data-post-id=\"11114748\" class=\"post-digg-count\">推荐(21)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10979808\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/06/05\">2019年6月5日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10979808.html\">\n",
      "    <span>\n",
      "        XGBoost算法原理小结\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10979808\">\n",
      "摘要：        \n",
      "在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P    <a href=\"https://www.cnblogs.com/pinard/p/10979808.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-06-05 20:36\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10979808\" class=\"post-view-count\">阅读(78728)</span>\n",
      "<span data-post-id=\"10979808\" class=\"post-comment-count\">评论(201)</span>\n",
      "<span data-post-id=\"10979808\" class=\"post-digg-count\">推荐(36)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10930902\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/05/27\">2019年5月27日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10930902.html\">\n",
      "    <span>\n",
      "        机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10930902\">\n",
      "摘要：        \n",
      "在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章    <a href=\"https://www.cnblogs.com/pinard/p/10930902.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-05-27 17:19\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10930902\" class=\"post-view-count\">阅读(43324)</span>\n",
      "<span data-post-id=\"10930902\" class=\"post-comment-count\">评论(27)</span>\n",
      "<span data-post-id=\"10930902\" class=\"post-digg-count\">推荐(7)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10825264\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/05/07\">2019年5月7日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10825264.html\">\n",
      "    <span>\n",
      "        机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10825264\">\n",
      "摘要：        \n",
      "在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果    <a href=\"https://www.cnblogs.com/pinard/p/10825264.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-05-07 15:59\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10825264\" class=\"post-view-count\">阅读(57439)</span>\n",
      "<span data-post-id=\"10825264\" class=\"post-comment-count\">评论(71)</span>\n",
      "<span data-post-id=\"10825264\" class=\"post-digg-count\">推荐(29)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10791506\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/04/29\">2019年4月29日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10791506.html\">\n",
      "    <span>\n",
      "        机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10791506\">\n",
      "摘要：        \n",
      "在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量    <a href=\"https://www.cnblogs.com/pinard/p/10791506.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-04-29 19:42\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10791506\" class=\"post-view-count\">阅读(41657)</span>\n",
      "<span data-post-id=\"10791506\" class=\"post-comment-count\">评论(84)</span>\n",
      "<span data-post-id=\"10791506\" class=\"post-digg-count\">推荐(19)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10773942\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/04/26\">2019年4月26日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10773942.html\">\n",
      "    <span>\n",
      "        机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10773942\">\n",
      "摘要：        \n",
      "在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导    <a href=\"https://www.cnblogs.com/pinard/p/10773942.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-04-26 18:42\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10773942\" class=\"post-view-count\">阅读(43307)</span>\n",
      "<span data-post-id=\"10773942\" class=\"post-comment-count\">评论(46)</span>\n",
      "<span data-post-id=\"10773942\" class=\"post-digg-count\">推荐(22)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10750718\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/04/22\">2019年4月22日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10750718.html\">\n",
      "    <span>\n",
      "        机器学习中的矩阵向量求导(一) 求导定义与求导布局\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10750718\">\n",
      "摘要：        \n",
      "在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的    <a href=\"https://www.cnblogs.com/pinard/p/10750718.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-04-22 18:03\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10750718\" class=\"post-view-count\">阅读(67913)</span>\n",
      "<span data-post-id=\"10750718\" class=\"post-comment-count\">评论(19)</span>\n",
      "<span data-post-id=\"10750718\" class=\"post-digg-count\">推荐(55)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10609228\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/03/27\">2019年3月27日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10609228.html\">\n",
      "    <span>\n",
      "        强化学习(十九) AlphaGo Zero强化学习原理\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10609228\">\n",
      "摘要：        \n",
      "在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG    <a href=\"https://www.cnblogs.com/pinard/p/10609228.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-03-27 20:11\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10609228\" class=\"post-view-count\">阅读(40187)</span>\n",
      "<span data-post-id=\"10609228\" class=\"post-comment-count\">评论(69)</span>\n",
      "<span data-post-id=\"10609228\" class=\"post-digg-count\">推荐(14)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10470571\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/03/04\">2019年3月4日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10470571.html\">\n",
      "    <span>\n",
      "        强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10470571\">\n",
      "摘要：        \n",
      "在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参    <a href=\"https://www.cnblogs.com/pinard/p/10470571.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-03-04 17:09\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10470571\" class=\"post-view-count\">阅读(50044)</span>\n",
      "<span data-post-id=\"10470571\" class=\"post-comment-count\">评论(29)</span>\n",
      "<span data-post-id=\"10470571\" class=\"post-digg-count\">推荐(5)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\t\t\n",
      "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_10384424\">\n",
      "    <div class=\"dayTitle\">\n",
      "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/02/15\">2019年2月15日\n",
      "</a>\n",
      "    </div>\n",
      "\n",
      "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
      "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/10384424.html\">\n",
      "    <span>\n",
      "        强化学习(十七) 基于模型的强化学习与Dyna算法框架\n",
      "    </span>\n",
      "    \n",
      "\n",
      "</a>\n",
      "        </div>\n",
      "        <div class=\"postCon\">\n",
      "\n",
      "<div class=\"c_b_p_desc\" id=\"postlist_description_10384424\">\n",
      "摘要：        \n",
      "在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy    <a href=\"https://www.cnblogs.com/pinard/p/10384424.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
      "</div>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "        <div class=\"postDesc\">posted @ 2019-02-15 20:22\n",
      "刘建平Pinard\n",
      "<span data-post-id=\"10384424\" class=\"post-view-count\">阅读(25644)</span>\n",
      "<span data-post-id=\"10384424\" class=\"post-comment-count\">评论(26)</span>\n",
      "<span data-post-id=\"10384424\" class=\"post-digg-count\">推荐(2)</span>\n",
      "\n",
      "</div>\n",
      "        <div class=\"clear\"></div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\n",
      "<div class=\"topicListFooter\">\n",
      "    <div id=\"nav_next_page\">\n",
      "        \n",
      "\n",
      "    </div>    \n",
      "</div>\n",
      "\n",
      "\n",
      "<div id=\"homepage_bottom_pager\" class=\"topicListFooter\">\n",
      "    \n",
      "\n",
      "<div class=\"pager\">\n",
      "    \n",
      "    \n",
      "    \n",
      "    1\n",
      "        <a href=\"https://www.cnblogs.com/pinard?page=2\">2</a>\n",
      "        <a href=\"https://www.cnblogs.com/pinard?page=3\">3</a>\n",
      "        <a href=\"https://www.cnblogs.com/pinard?page=4\">4</a>\n",
      "        <a href=\"https://www.cnblogs.com/pinard?page=5\">5</a>\n",
      "    ···\n",
      "    <a href=\"https://www.cnblogs.com/pinard?page=14\">14</a>\n",
      "    <a href=\"https://www.cnblogs.com/pinard?page=2\">下一页</a>\n",
      "</div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\n",
      "<script>\n",
      "    updatePostStats(\n",
      "        $(\".post-view-count\").map(function () { return this.dataset.postId }).get(),\n",
      "        function(id, count) { $(\".post-view-count[data-post-id=\" + id + \"]\").text(\"阅读(\" + count + \")\") },\n",
      "        function(id, count) { $(\".post-comment-count[data-post-id=\" + id + \"]\").text(\"评论(\" + count + \")\") },\n",
      "        function(id, count) { $(\".post-digg-count[data-post-id=\" + id + \"]\").text(\"推荐(\" + count + \")\") });\n",
      "</script>\n",
      "\t</div>\n",
      "\t</div>\n",
      "\t<div id=\"sideBar\">\n",
      "\t\t<div id=\"sideBarMain\">\n",
      "\t\t\t<div id=\"sidebar_news\" class=\"newsItem\">\n",
      "    <h3 class=\"catListTitle\">公告</h3>\n",
      "<div id=\"blog-news\" class=\"sidebar-news\">\n",
      "    <div id=\"sidebar_news_container\">\n",
      "    </div>\n",
      "</div>\n",
      "<script>loadBlogNews();</script>\n",
      "\n",
      " \n",
      "</div>\n",
      "    <div id=\"sidebar_c3\">\n",
      "        <a href=\"https://qoder.com/\" target=\"_blank\" onclick=\"countCreativeClicks('C3-Qoder')\">\n",
      "            <img src=\"https://img2024.cnblogs.com/blog/35695/202510/35695-20251013111242110-2029482428.jpg\" onload=\"countCreativeImpressions('C3-Qoder')\" />\n",
      "        </a>\n",
      "    </div>\n",
      "\n",
      "\t\t\t<div id=\"blog-calendar\" style=\"display:none\"></div><script>loadBlogDefaultCalendar();</script>\t\t\t\n",
      "\t\t\t<div id=\"leftcontentcontainer\">\n",
      "\t\t\t\t<div id=\"blog-sidecolumn\"></div>\n",
      "                    <script>loadBlogSideColumn();</script>\n",
      "\t\t\t</div>\t\t\t\n",
      "\t\t</div>\n",
      "\t</div>\n",
      "\t<div class=\"clear\"></div>\n",
      "\t</div>\n",
      "\t<div class=\"clear\"></div>\n",
      "\t<div id=\"footer\">\n",
      "\t\t<a href=\"https://www.cnblogs.com/\" id=\"footer_site_link\" class=\"footer-brand\">博客园</a>\n",
      "<span class=\"footer-copyright\"> &nbsp;&copy;&nbsp; 2004-2025</span>\n",
      "<br /><span class=\"footer-icp\">\n",
      "    <a target=\"_blank\" href=\"http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33010602011771\"><img src=\"//assets.cnblogs.com/images/ghs.png\" />浙公网安备 33010602011771号</a>\n",
      "    <a href=\"https://beian.miit.gov.cn\" target=\"_blank\">浙ICP备2021040463号-3</a>\n",
      "</span>\n",
      "\n",
      "\n",
      "\n",
      "\t</div>\n",
      "</div>\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    <input type=\"hidden\" id=\"antiforgery_token\" value=\"CfDJ8G33EJ5dWE5OhU_7yPrjq1-rKc50xHUqbyQQEcUyNe1GzsIfFjRNU3eEvyhDVS40LVmBzY7bsxZOX1irikfgd4ASyHWbQzXKNzrYo78_UNJ1gU-QIGlff-prZamNcuU-r_F8ZyyEaHcwtpCH7xPvzd0\" />\n",
      "    <script async src=\"https://www.googletagmanager.com/gtag/js?id=G-M95P3TTWJZ\"></script>\n",
      "<script>\n",
      "    window.dataLayer = window.dataLayer || [];\n",
      "    function gtag() { dataLayer.push(arguments); }\n",
      "    gtag('js', new Date());\n",
      "    gtag('config', 'G-M95P3TTWJZ');\n",
      "</script>\n",
      "<script defer src=\"https://hm.baidu.com/hm.js?866c9be12d4a814454792b1fd0fed295\"></script>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from lxml import etree         # 用于解析 HTML 文档\n",
    "from bs4 import BeautifulSoup  # CSS 选择器\n",
    "\"\"\"\n",
    "1、明确需要爬的内容：标题，时间，阅读量，评论数，推荐数，摘要\n",
    "2、用Edge浏览器打开需要爬虫的网址，右键-检查-网络-清除网络日志-重新加载页面-点“名称”下面的内容-预览(判断我们需要爬取的内容在不在里面)\n",
    "   -标头(确定请求URL，请求方法（GET，POST），请求参数，请求标头，Cookies，User-Agent信息)-翻页\n",
    "\"\"\"\n",
    "\n",
    "# 检查网站是否有反爬功能（博客园没有）\n",
    "url = \"https://www.cnblogs.com/pinard?page=1\"\n",
    "res = requests.get(url)\n",
    "print(f\"状态码：{res.status_code}\")\n",
    "print(f\"网页编码：{res.encoding}\\n\")\n",
    "print(f\"网页源代码：\\n{res.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a92cbe-d39a-4001-9ebb-015db1943029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 URL 列表\n",
    "urls = [f'https://www.cnblogs.com/pinard/default.html?page={page}' for page in range(1, 15)]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b866eca-88d5-42d8-ae82-4d91fa6172f6",
   "metadata": {},
   "source": [
    "## 方法一：正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1dd3a0-4504-4d95-8f23-a0ea682bd0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!-- 源代码 -->\n",
    "<div class=\"day\" role=\"article\" aria-describedby=\"postlist_description_11114748\">\n",
    "    <div class=\"dayTitle\">\n",
    "        <a href=\"https://www.cnblogs.com/pinard/p/archive/2019/07/01\">2019年7月1日\n",
    "</a>\n",
    "    </div>\n",
    "\n",
    "        <div class=\"postTitle\" role=\"heading\" aria-level=\"2\">\n",
    "            <a class=\"postTitle2 vertical-middle\" href=\"https://www.cnblogs.com/pinard/p/11114748.html\">\n",
    "    <span>\n",
    "        XGBoost类库使用小结\n",
    "    </span>\n",
    "</a>\n",
    "        </div>\n",
    "        <div class=\"postCon\">\n",
    "\n",
    "<div class=\"c_b_p_desc\" id=\"postlist_description_11114748\">\n",
    "摘要：        \n",
    "在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth    <a href=\"https://www.cnblogs.com/pinard/p/11114748.html\" class=\"c_b_p_desc_readmore\">阅读全文</a>\n",
    "</div>\n",
    "</div>\n",
    "        <div class=\"clear\"></div>\n",
    "        <div class=\"postDesc\">posted @ 2019-07-01 18:10\n",
    "刘建平Pinard\n",
    "<span data-post-id=\"11114748\" class=\"post-view-count\">阅读(66627)</span>\n",
    "<span data-post-id=\"11114748\" class=\"post-comment-count\">评论(135)</span>\n",
    "<span data-post-id=\"11114748\" class=\"post-digg-count\">推荐(21)</span>\n",
    "</div>\n",
    "        <div class=\"clear\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c362dc1-69d9-4e74-bcfb-71cc420c1846",
   "metadata": {},
   "source": [
    "提取 1 个 URL 中的内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb53b68-1783-4648-9f76-963ff7ea6da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost类库使用小结 2019年7月1日 66952 135 21         在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth     \n",
      "\n",
      "XGBoost算法原理小结 2019年6月5日 78728 201 36         在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P     \n",
      "\n",
      "机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导 2019年5月27日 43324 27 7         在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章     \n",
      "\n",
      "机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则 2019年5月7日 57439 71 29         在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果     \n",
      "\n",
      "机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法 2019年4月29日 41657 84 19         在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量     \n",
      "\n",
      "机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法 2019年4月26日 43307 46 22         在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导     \n",
      "\n",
      "机器学习中的矩阵向量求导(一) 求导定义与求导布局 2019年4月22日 67913 19 55         在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的     \n",
      "\n",
      "强化学习(十九) AlphaGo Zero强化学习原理 2019年3月27日 40187 69 14         在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG     \n",
      "\n",
      "强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS) 2019年3月4日 50044 29 5         在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参     \n",
      "\n",
      "强化学习(十七) 基于模型的强化学习与Dyna算法框架 2019年2月15日 25644 26 2         在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy     \n",
      "\n",
      "文章数量：10\n"
     ]
    }
   ],
   "source": [
    "# 初始化文章数量\n",
    "count = 0\n",
    "\n",
    "# 发送 GET 请求，获取网页内容\n",
    "res = requests.get(url)\n",
    "res.encoding = 'utf-8'\n",
    "# (.*?)无法匹配换行符，所以要先去除换行符\n",
    "str_text = res.text.replace(\"\\n\", \"\")\n",
    "\n",
    "# 保存原始 HTML 内容\n",
    "# 因有反爬功能存在，返回的内容和我们看到的内容可能不一致，需要保存下来看一下\n",
    "with open('./output/cnblogs.html', \"w\", encoding='utf-8') as f:\n",
    "    f.write(str_text)\n",
    "\n",
    "# 提取博客条目\n",
    "# 开始部分：<div class=\"day\"\n",
    "# 提取部分：(.*?)\n",
    "# 结尾部分：<div class=\"clear\"></div></div>\n",
    "blogs = re.findall(r'<div class=\"day\"(.*?)<div class=\"clear\"></div></div>', str_text)\n",
    "# print(len(blogs), blogs)\n",
    "\n",
    "# 解析每个博客的详细信息\n",
    "for blog in blogs:\n",
    "    # 标题，时间，阅读量，评论数，推荐数，摘要\n",
    "    title = re.findall(r'<span>(.*?)</span>', blog)[0].strip()\n",
    "    date = re.findall(r'<div class=\"dayTitle\">.*?>(.*?)</a>', blog)[0]  # .*?>：匹配 <a href=\"...\">，包括前面的空格\n",
    "    read = re.findall(r'阅读\\((.*?)\\)', blog)[0]\n",
    "    comment = re.findall(r'评论\\((.*?)\\)', blog)[0]\n",
    "    recommendation = re.findall(r'推荐\\((.*?)\\)', blog)[0]\n",
    "    abstract = re.findall(r'摘要：(.*?)<a', blog)[0]\n",
    "    print(title, date, read, comment, recommendation, abstract, \"\\n\")\n",
    "\n",
    "    # 连接成一个字符串\n",
    "    strs = f\"{title}\\t{date}\\t{read}\\t{comment}\\t{recommendation}\\t{abstract}\"\n",
    "    # print(strs)\n",
    "    \n",
    "    # 以追加模式将字符串写入文件，每行一个博客记录\n",
    "    with open('./output/cnblogs_datas.txt', 'a', encoding='utf-8') as f:\n",
    "        f.write(strs + '\\n')\n",
    "    count += 1\n",
    "        \n",
    "print(f\"文章数量：{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd8c8f-ce53-44ec-976f-42cc88c7bce1",
   "metadata": {},
   "source": [
    "提取多个 URL 中的内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10d7883-693b-43ad-a34b-6016332b3048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost类库使用小结 2019年7月1日 66627 135 21         在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth     \n",
      "\n",
      "XGBoost算法原理小结 2019年6月5日 78411 201 36         在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P     \n",
      "\n",
      "机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导 2019年5月27日 43132 27 7         在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章     \n",
      "\n",
      "机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则 2019年5月7日 57167 71 29         在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果     \n",
      "\n",
      "机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法 2019年4月29日 41449 84 19         在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量     \n",
      "\n",
      "机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法 2019年4月26日 43034 46 22         在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导     \n",
      "\n",
      "机器学习中的矩阵向量求导(一) 求导定义与求导布局 2019年4月22日 67509 19 55         在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的     \n",
      "\n",
      "强化学习(十九) AlphaGo Zero强化学习原理 2019年3月27日 39821 69 14         在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG     \n",
      "\n",
      "强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS) 2019年3月4日 49710 29 5         在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参     \n",
      "\n",
      "强化学习(十七) 基于模型的强化学习与Dyna算法框架 2019年2月15日 25460 26 2         在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy     \n",
      "\n",
      "强化学习(十六) 深度确定性策略梯度(DDPG) 2019年2月1日 123501 318 24         在强化学习(十五) A3C中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Poli     \n",
      "\n",
      "强化学习(十五) A3C 2019年1月29日 71528 144 4         在强化学习(十四) Actor-Critic中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C     \n",
      "\n",
      "强化学习(十四) Actor-Critic 2019年1月15日 114927 148 9         在强化学习(十三) 策略梯度(Policy Gradient)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。 在本篇我们讨论策略(Policy     \n",
      "\n",
      "强化学习(十三) 策略梯度(Policy Gradient) 2018年12月18日 122592 177 14         在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradien     \n",
      "\n",
      "强化学习(十二) Dueling DQN 2018年11月8日 58435 74 5         在强化学习(十一) Prioritized Replay DQN中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的deep RL tutorial和Dueling DQN的论文&lt;Dueling N     \n",
      "\n",
      "强化学习(十一) Prioritized Replay DQN 2018年10月16日 52696 153 14         在强化学习（十）Double DQN (DDQN)中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay      \n",
      "\n",
      "强化学习（十）Double DQN (DDQN) 2018年10月12日 108696 77 8         在强化学习（九）Deep Q-Learning进阶之Nature DQN中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称D     \n",
      "\n",
      "强化学习（九）Deep Q-Learning进阶之Nature DQN 2018年10月8日 69415 84 12         在强化学习（八）价值函数的近似表示与Deep Q-Learning中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 201     \n",
      "\n",
      "强化学习（八）价值函数的近似表示与Deep Q-Learning 2018年9月28日 89548 203 13         在强化学习系列的前七篇里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。 Deep&#160;Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。 1. 为何需要价值函数     \n",
      "\n",
      "强化学习（七）时序差分离线控制算法Q-Learning 2018年9月19日 61517 109 15         在强化学习（六）时序差分在线控制算法SARSA中我们讨论了时序差分的在线控制算法SARSA，而另一类时序差分的离线控制算法还没有讨论，因此本文我们关注于时序差分离线控制算法，主要是经典的Q-Learning算法。 Q-Learning这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部     \n",
      "\n",
      "强化学习（六）时序差分在线控制算法SARSA 2018年9月9日 60728 87 10         在强化学习（五）用时序差分法（TD）求解中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。 SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。 1.&#160;SARSA算法的引入 S     \n",
      "\n",
      "强化学习（五）用时序差分法（TD）求解 2018年8月24日 77616 131 16         在强化学习（四）用蒙特卡罗法（MC）求解中，我们讲到了使用蒙特卡罗法来求解强化学习问题的方法，虽然蒙特卡罗法很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果我们没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。本文我们就来讨论可以不使用完整状态序列求解强化     \n",
      "\n",
      "强化学习（四）用蒙特卡罗法（MC）求解 2018年8月17日 73175 108 18         在强化学习（三）用动态规划（DP）求解中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型$P$都无法知道，这时动态规划法根本没法     \n",
      "\n",
      "强化学习（三）用动态规划（DP）求解 2018年8月12日 74303 103 22         在强化学习（二）马尔科夫决策过程(MDP)中，我们讨论了用马尔科夫假设来简化强化学习模型的复杂度，这一篇我们在马尔科夫假设和贝尔曼方程的基础上讨论使用动态规划(Dynamic Programming, DP)来求解强化学习的问题。 动态规划这一篇对应Sutton书的第四章和UCL强化学习课程的第三讲     \n",
      "\n",
      "强化学习（二）马尔科夫决策过程(MDP) 2018年8月5日 159034 142 26         在强化学习（一）模型基础中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。 MDP     \n",
      "\n",
      "强化学习（一）模型基础 2018年7月29日 153998 75 36         从今天开始整理强化学习领域的知识，主要参考的资料是Sutton的强化学习书和UCL强化学习的课程。这个系列大概准备写10到20篇，希望写完后自己的强化学习碎片化知识可以得到融会贯通，也希望可以帮到更多的人，毕竟目前系统的讲解强化学习的中文资料不太多。 第一篇会从强化学习的基本概念讲起，对应Sutto     \n",
      "\n",
      "异常点检测算法小结 2018年7月15日 52156 78 11         异常点检测，有时也叫离群点检测，英文一般叫做Novelty Detection或者Outlier Detection,是比较常见的一类非监督学习算法，这里就对异常点检测算法做一个总结。 1.&#160;异常点检测算法使用场景 什么时候我们需要异常点检测算法呢？常见的有三种情况。一是在做特征工程的时候需要对异常     \n",
      "\n",
      "tensorflow机器学习模型的跨平台上线 2018年7月1日 14256 18 2         在用PMML实现机器学习模型的跨平台上线中，我们讨论了使用PMML文件来实现跨平台模型上线的方法，这个方法当然也适用于tensorflow生成的模型，但是由于tensorflow模型往往较大，使用无法优化的PMML文件大多数时候很笨拙，因此本文我们专门讨论下tensorflow机器学习模型的跨平台上     \n",
      "\n",
      "用PMML实现机器学习模型的跨平台上线 2018年6月24日 43199 79 15         在机器学习用于产品的时候，我们经常会遇到跨平台的问题。比如我们用Python基于一系列的机器学习库训练了一个模型，但是有时候其他的产品和项目想把这个模型集成进去，但是这些产品很多只支持某些特定的生产环境比如Java，为了上一个机器学习模型去大动干戈修改环境配置很不划算，此时我们就可以考虑用预测模型标     \n",
      "\n",
      "用tensorflow学习贝叶斯个性化排序(BPR) 2018年6月10日 21476 46 5         在贝叶斯个性化排序(BPR)算法小结中，我们对贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)的原理做了讨论，本文我们将从实践的角度来使用BPR做一个简单的推荐。由于现有主流开源类库都没有BPR，同时它又比较简单，因此用tensorflow自己实现一个     \n",
      "\n",
      "贝叶斯个性化排序(BPR)算法小结 2018年6月3日 48331 68 16         在矩阵分解在协同过滤推荐算法中的应用中，我们讨论过像funkSVD之类的矩阵分解方法如何用于推荐。今天我们讲另一种在实际产品中用的比较多的推荐算法:贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)，它也用到了矩阵分解，但是和funkSVD家族却有很多不     \n",
      "\n",
      "特征工程之特征预处理 2018年5月26日 31583 113 27         在前面我们分别讨论了特征工程中的特征选择与特征表达，本文我们来讨论特征预处理的相关问题。主要包括特征的归一化和标准化，异常特征样本清洗与样本数据不平衡问题的处理。 1. 特征的标准化和归一化 由于标准化和归一化这两个词经常混用，所以本文不再区别标准化和归一化，而通过具体的标准化和归一化方法来区别具体     \n",
      "\n",
      "特征工程之特征表达 2018年5月19日 28253 107 12         在特征工程之特征选择中，我们讲到了特征选择的一些要点。本篇我们继续讨论特征工程，不过会重点关注于特征表达部分，即如果对某一个特征的具体表现形式做处理。主要包括缺失值处理，特殊的特征处理比如时间和地理位置处理，离散特征的连续化和离散化处理，连续特征的离散化处理几个方面。 1. 缺失值处理 特征有缺失值     \n",
      "\n",
      "特征工程之特征选择 2018年5月13日 57252 115 31         特征工程是数据分析中最耗时间和精力的一部分工作，它不像算法和模型那样是确定的步骤，更多是工程上的经验和权衡。因此没有统一的方法。这里只是对一些常用的方法做一个总结。本文关注于特征选择部分。后面还有两篇会关注于特征表达和特征预处理。 1. 特征的来源 在做数据分析的时候，特征的来源一般有两块，一块是业     \n",
      "\n",
      "用gensim学习word2vec 2017年8月3日 95553 104 24         在word2vec原理篇中，我们对word2vec的两种模型CBOW和Skip-Gram，以及两种解法Hierarchical Softmax和Negative Sampling做了总结。这里我们就从实践的角度，使用gensim来学习word2vec。 1. gensim安装与概述 gensim是一     \n",
      "\n",
      "word2vec原理(三) 基于Negative Sampling的模型 2017年7月28日 105869 131 27         word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 在上一篇中我们讲到了基于Hierarchical Softmax的word2ve     \n",
      "\n",
      "word2vec原理(二) 基于Hierarchical Softmax的模型 2017年7月27日 137786 290 45         word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 在word2vec原理(一) CBOW与Skip-Gram模型基础中，我们讲到了     \n",
      "\n",
      "word2vec原理(一) CBOW与Skip-Gram模型基础 2017年7月13日 254233 112 44         word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 word2vec是google在2013年推出的一个NLP工具，它的特点是将所有     \n",
      "\n",
      "条件随机场CRF(三) 模型学习与维特比算法解码 2017年6月23日 28614 66 7         条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三)&#160;模型学习与维特比算法解码 在CRF系列的前两篇，我们总结了CRF的模型基础与第一个问题的求解方法，本文我们关注于linear-CRF的第二个问题与第三个问题的求解。第二个问     \n",
      "\n",
      "条件随机场CRF(二) 前向后向算法评估标记序列概率 2017年6月22日 23180 80 5         条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 在条件随机场CRF(一)中我们总结了CRF的模型，主要是linear-CRF的模型原理。本文就继续讨论linear-CRF需要解决的三个问题：评估     \n",
      "\n",
      "条件随机场CRF(一)从随机场到线性链条件随机场 2017年6月19日 54902 99 21         条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 条件随机场(Conditional Random Fields, 以下简称CRF)是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然     \n",
      "\n",
      "用hmmlearn学习隐马尔科夫模型HMM 2017年6月13日 54977 190 15         在之前的HMM系列中，我们对隐马尔科夫模型HMM的原理以及三个问题的求解方法做了总结。本文我们就从实践的角度用Python的hmmlearn库来学习HMM的使用。关于hmmlearn的更多资料在官方文档有介绍。 1.&#160;hmmlearn概述 hmmlearn安装很简单，&quot;pip install hmm     \n",
      "\n",
      "隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 2017年6月12日 43414 35 8         隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在本篇我们会讨论HMM模型最后一个问题的求解，即即给定模型和观测序列，求给定观测序列条件下，最     \n",
      "\n",
      "隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 2017年6月10日 40495 97 12         隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在本篇我们会讨论HMM模型参数求解的问题，这个问题在HMM三个问题里算是最复杂的。在研究这个问     \n",
      "\n",
      "隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 2017年6月8日 63219 61 20         隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在隐马尔科夫模型HMM（一）HMM模型中，我们讲到了HMM模型的基础知识和HMM的三个基本问题     \n",
      "\n",
      "隐马尔科夫模型HMM（一）HMM模型 2017年6月6日 119704 37 35         隐马尔科夫模型HMM（一）HMM模型基础 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学     \n",
      "\n",
      "EM算法原理总结 2017年5月27日 103419 131 33         EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。本文就对EM算法的原理做一个总结。 1.&#160;EM算法要解决的问题 我们经常会从样本观察数据中，找出样本的模型参     \n",
      "\n",
      "用scikit-learn学习LDA主题模型 2017年5月26日 67781 115 13         在LDA模型原理篇我们总结了LDA主题模型的原理，这里我们就从应用的角度来使用scikit-learn来学习LDA主题模型。除了scikit-learn, 还有spark MLlib和gensim库也有LDA主题模型的类库，使用的原理基本类似，本文关注于scikit-learn中LDA主题模型的使用     \n",
      "\n",
      "文本主题模型之LDA(三) LDA求解之变分推断EM算法 2017年5月22日 32770 101 11         文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 本文是LDA主题模型的第三篇，读这一篇之前建议先读文本主题模型之LDA(一) LDA基础，同时由于使用了EM算法，如果你对EM算法不熟悉，建议     \n",
      "\n",
      "文本主题模型之LDA(二) LDA求解之Gibbs采样算法 2017年5月18日 55342 218 10         文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 本文是LDA主题模型的第二篇，读这一篇之前建议先读文本主题模型之LDA(一) LDA基础，同时由于使用了基于MCMC的Gibbs采样算法，如果     \n",
      "\n",
      "文本主题模型之LDA(一) LDA基础 2017年5月17日 214115 105 29         文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 在前面我们讲到了基于矩阵分解的LSI和NMF主题模型，这里我们开始讨论被广泛使用的主题模型：隐含狄利克雷分布(Latent Dirichlet     \n",
      "\n",
      "文本主题模型之非负矩阵分解(NMF) 2017年5月5日 31284 15 9         在文本主题模型之潜在语义索引(LSI)中，我们讲到LSI主题模型使用了奇异值分解，面临着高维度计算量太大的问题。这里我们就介绍另一种基于矩阵分解的主题模型：非负矩阵分解(NMF)，它同样使用了矩阵分解，但是计算量和处理速度则比LSI快，它是怎么做到的呢？ 1.&#160;非负矩阵分解(NMF)概述 非负矩阵分     \n",
      "\n",
      "文本主题模型之潜在语义索引(LSI) 2017年5月4日 40489 48 11         在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 1. 文本主题模型的问题特点 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型     \n",
      "\n",
      "英文文本挖掘预处理流程总结 2017年4月24日 27689 16 9         在中文文本挖掘预处理流程总结中，我们总结了中文文本挖掘的预处理流程，这里我们再对英文文本挖掘的预处理流程做一个总结。 1. 英文文本挖掘预处理特点 英文文本的预处理方法和中文的有部分区别。首先，英文文本挖掘预处理一般可以不做分词（特殊需求除外），而中文预处理分词是必不可少的一步。第二点，大部分英文文     \n",
      "\n",
      "中文文本挖掘预处理流程总结 2017年4月21日 59109 76 20         在对文本做数据分析时，我们一大半的时间都会花在文本预处理上，而中文和英文的预处理流程稍有不同，本文就对中文文本挖掘的预处理流程做一个总结。 1. 中文文本挖掘预处理特点 首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。 首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接     \n",
      "\n",
      "文本挖掘预处理之TF-IDF 2017年4月11日 77670 39 20         在文本挖掘预处理之向量化与Hash Trick中我们讲到在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。 1. 文本向量化特征的不足 在将文本分词并向量化后，我们可以得到词汇表中每个词在各     \n",
      "\n",
      "文本挖掘预处理之向量化与Hash Trick 2017年4月10日 28224 36 8         在文本挖掘的分词原理中，我们讲到了文本挖掘的预处理的关键一步：“分词”，而在做了分词后，如果我们是做文本分类聚类，则后面关键的特征预处理步骤有向量化或向量化的特例Hash Trick，本文我们就对向量化和特例Hash Trick预处理方法做一个总结。 1. 词袋模型 在讲向量化与Hash Trick     \n",
      "\n",
      "文本挖掘的分词原理 2017年4月7日 37934 65 17         在做文本挖掘的时候，首先要做的预处理就是分词。英文单词天然有空格隔开容易按照空格分词，但是也有时候需要把多个单词做为一个分词，比如一些名词如“New York”，需要做为一个词看待。而中文由于没有空格，分词就是一个需要专门去解决的问题了。无论是英文还是中文，分词的原理都是类似的，本文就对文本挖掘时的     \n",
      "\n",
      "MCMC(四)Gibbs采样 2017年3月30日 101877 145 30         MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(三)MCMC采样和M-H采样中，我们讲到了M-H采样已经可以很好的解决蒙特卡罗方法需要的任意概率分布的样本集的问题。但是M-H采样有两个缺点：一是需要计算接受率，在     \n",
      "\n",
      "MCMC(三)MCMC采样和M-H采样 2017年3月29日 122198 267 34         MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(二)马尔科夫链中我们讲到给定一个概率平稳分布$\\pi$, 很难直接找到对应的马尔科夫链状态转移矩阵$P$。而只要解决这个问题，我们就可以找到一种通用的概率分布采样方     \n",
      "\n",
      "MCMC(二)马尔科夫链 2017年3月28日 120423 150 49         MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(一)蒙特卡罗方法中，我们讲到了如何用蒙特卡罗方法来随机模拟求解一些复杂的连续积分或者离散求和的方法，但是这个方法需要得到对应的概率分布的样本集，而想得到这样的样本集     \n",
      "\n",
      "MCMC(一)蒙特卡罗方法 2017年3月27日 181426 116 69         MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 作为一种随机采样方法，马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，以下简称MCMC）在机器学习,深度学习以及自然语言处理等领域都有广泛的应用，是很多复     \n",
      "\n",
      "受限玻尔兹曼机（RBM）原理总结 2017年3月11日 48466 44 17         在前面我们讲到了深度学习的两类神经网络模型的原理，第一类是前向的神经网络，即DNN和CNN。第二类是有反馈的神经网络，即RNN和LSTM。今天我们就总结下深度学习里的第三类神经网络模型：玻尔兹曼机。主要关注于这类模型中的受限玻尔兹曼机（Restricted Boltzmann Machine，以下简     \n",
      "\n",
      "LSTM模型与前向反向传播算法 2017年3月8日 95172 173 34         在循环神经网络(RNN)模型与前向反向传播算法中，我们总结了对RNN模型做了总结。由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。     \n",
      "\n",
      "循环神经网络(RNN)模型与前向反向传播算法 2017年3月6日 156117 217 28         在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。今天我们就讨论另一类输出和模型间有反馈的神经网络：循环神经网络(Recurrent Neural Networks ，以下简称RNN)，它广泛的用于自然语言处理中的语音识     \n",
      "\n",
      "卷积神经网络(CNN)反向传播算法 2017年3月3日 204289 256 61         在卷积神经网络(CNN)前向传播算法中，我们对CNN的前向传播算法做了总结，基于CNN前向传播算法的基础，我们下面就对CNN的反向传播算法做一个总结。在阅读本文前，建议先研究DNN的反向传播算法：深度神经网络（DNN）反向传播算法(BP) 1. 回顾DNN的反向传播算法 我们首先回顾DNN的反向传播     \n",
      "\n",
      "卷积神经网络(CNN)前向传播算法 2017年3月2日 70467 56 17         在卷积神经网络(CNN)模型结构中，我们对CNN的模型结构做了总结，这里我们就在CNN的模型基础上，看看CNN的前向传播算法是什么样子的。重点会和传统的DNN比较讨论。 1. 回顾CNN的结构 在上一篇里，我们已经讲到了CNN的结构，包括输出层，若干的卷积层+ReLU激活函数，若干的池化层，DNN全     \n",
      "\n",
      "卷积神经网络(CNN)模型结构 2017年3月1日 211701 79 32         在前面我们讲述了DNN的模型与前向反向传播算法。而在DNN大类中，卷积神经网络(Convolutional Neural Networks，以下简称CNN)是最为成功的DNN特例之一。CNN广泛的应用于图像识别，当然现在也应用于NLP等其他领域，本文我们就对CNN的模型结构做一个总结。 在学习CNN     \n",
      "\n",
      "深度神经网络（DNN）的正则化 2017年2月27日 40948 40 13         和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，这里我们就对DNN的正则化方法做一个总结。 1. DNN的L1&amp;L2正则化 想到正则化，我们首先想到的就是L1正则化和L2正则化。L1正则化和L2正则化原理类似，这里重点讲述DNN的L2正则化。 而DNN的L2正则化通常的做法是只针     \n",
      "\n",
      "深度神经网络（DNN）损失函数和激活函数的选择 2017年2月24日 87075 164 15         在深度神经网络（DNN）反向传播算法(BP)中，我们对DNN的前向反向传播算法的使用做了总结。里面使用的损失函数是均方差，而激活函数是Sigmoid。实际上DNN可以使用的损失函数和激活函数不少。这些损失函数和激活函数如何选择呢？下面我们就对DNN损失函数和激活函数的选择做一个总结。 1. 均方差损     \n",
      "\n",
      "深度神经网络（DNN）反向传播算法(BP) 2017年2月21日 132772 176 40         在深度神经网络（DNN）模型与前向传播算法中，我们对DNN的模型和前向传播算法做了总结，这里我们更进一步，对DNN的反向传播算法（Back Propagation，BP）做一个总结。 1. DNN反向传播算法要解决的问题 在了解DNN的反向传播算法前，我们先要知道DNN反向传播算法要解决的问题，也就     \n",
      "\n",
      "深度神经网络（DNN）模型与前向传播算法 2017年2月20日 222483 47 52         深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。 1. 从感知机到神经网络 在感知机原理小结中，我们介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:     \n",
      "\n",
      "分解机(Factorization Machines)推荐算法原理 2017年2月6日 51981 72 11         对于分解机(Factorization Machines，FM)推荐算法原理，本来想自己单独写一篇的。但是看到peghoty写的FM不光简单易懂，而且排版也非常好，因此转载过来，自己就不再单独写FM了。 Pinard注：上面最后一句话应该是&quot;而$g_{\\theta}(x)$则利用$\\widehat{     \n",
      "\n",
      "用Spark学习矩阵分解推荐算法 2017年2月4日 21894 44 6         在矩阵分解在协同过滤推荐算法中的应用中，我们对矩阵分解在推荐算法中的应用原理做了总结，这里我们就从实践的角度来用Spark学习矩阵分解推荐算法。 1. Spark推荐算法概述 在Spark MLlib中，推荐算法这块只实现了基于矩阵分解的协同过滤推荐算法。而基于的算法是FunkSVD算法，即将m个用     \n",
      "\n",
      "SimRank协同过滤推荐算法 2017年2月3日 19971 36 5         在协同过滤推荐算法总结中，我们讲到了用图模型做协同过滤的方法，包括SimRank系列算法和马尔科夫链系列算法。现在我们就对SimRank算法在推荐系统的应用做一个总结。 1.&#160;SimRank推荐算法的图论基础 SimRank是基于图论的，如果用于推荐算法，则它假设用户和物品在空间中形成了一张图。而这     \n",
      "\n",
      "矩阵分解在协同过滤推荐算法中的应用 2017年1月26日 48077 86 19         在协同过滤推荐算法总结中，我们讲到了用矩阵分解做协同过滤是广泛使用的方法，这里就对矩阵分解在协同过滤推荐算法中的应用做一个总结。(过年前最后一篇！祝大家新年快乐！明年的目标是写120篇机器学习，深度学习和NLP相关的文章) 1.&#160;矩阵分解用于推荐算法要解决的问题 在推荐系统中，我们常常遇到的问题是这     \n",
      "\n",
      "协同过滤推荐算法总结 2017年1月25日 95413 66 41         推荐算法具有非常多的应用场景和商业价值，因此对推荐算法值得好好研究。推荐算法种类很多，但是目前应用最广泛的应该是协同过滤类别的推荐算法，本文就对协同过滤类别的推荐算法做一个概括总结，后续也会对一些典型的协同过滤推荐算法做原理总结。 1. 推荐算法概述 推荐算法是非常古老的，在机器学习还没有兴起的时候     \n",
      "\n",
      "用Spark学习FP Tree算法和PrefixSpan算法 2017年1月22日 14262 24 4         在FP Tree算法原理总结和PrefixSpan算法原理总结中，我们对FP Tree和PrefixSpan这两种关联算法的原理做了总结，这里就从实践的角度介绍如何使用这两个算法。由于scikit-learn中没有关联算法的类库，而Spark MLlib有，本文的使用以Spark MLlib作为使用     \n",
      "\n",
      "PrefixSpan算法原理总结 2017年1月20日 39662 32 8         前面我们讲到频繁项集挖掘的关联算法Apriori和FP Tree。这两个算法都是挖掘频繁项集的。而今天我们要介绍的PrefixSpan算法也是关联算法，但是它是挖掘频繁序列模式的，因此要解决的问题目标稍有不同。 1.&#160;项集数据和序列数据 首先我们看看项集数据和序列数据有什么不同，如下图所示。 左边的     \n",
      "\n",
      "FP Tree算法原理总结 2017年1月19日 84631 80 48         在Apriori算法原理总结中，我们对Apriori算法的原理做了总结。作为一个挖掘频繁项集的算法，Apriori算法需要多次扫描数据，I/O是很大的瓶颈。为了解决这个问题，FP Tree算法（也称FP Growth算法）采用了一些技巧，无论多少数据，只需要扫描两次数据集，因此提高了算法运行的效率。     \n",
      "\n",
      "Apriori算法原理总结 2017年1月17日 123648 52 31         Apriori算法是常用的用于挖掘出数据关联规则的算法，它用来找出数据值中频繁出现的数据集合，找出这些集合的模式有助于我们做一些决策。比如在常见的超市购物数据集，或者电商的网购数据集中，如果我们找到了频繁出现的数据集，那么对于超市，我们可以优化产品的位置摆放，对于电商，我们可以优化商品所在的仓库位置     \n",
      "\n",
      "典型关联分析(CCA)原理总结 2017年1月16日 69993 59 20         典型关联分析(Canonical Correlation Analysis，以下简称CCA)是最常用的挖掘数据关联关系的算法之一。比如我们拿到两组数据，第一组是人身高和体重的数据，第二组是对应的跑步能力和跳远能力的数据。那么我们能不能说这两组数据是相关的呢？CCA可以帮助我们分析这个问题。 1. C     \n",
      "\n",
      "用scikit-learn研究局部线性嵌入(LLE) 2017年1月11日 11553 0 5         在局部线性嵌入(LLE)原理总结中，我们对流形学习中的局部线性嵌入(LLE)算法做了原理总结。这里我们就对scikit-learn中流形学习的一些算法做一个介绍，并着重对其中LLE算法的使用方法做一个实践上的总结。 1.&#160;scikit-learn流形学习库概述 在scikit-learn中，流形学习     \n",
      "\n",
      "局部线性嵌入(LLE)原理总结 2017年1月10日 68070 94 15         局部线性嵌入(Locally Linear Embedding，以下简称LLE)也是非常重要的降维方法。和传统的PCA，LDA等关注样本方差的降维方法相比，LLE关注于降维时保持样本局部的线性特征，由于LLE在降维时保持了样本的局部特征，它广泛的用于图像图像识别，高维数据可视化等领域。下面我们就对L     \n",
      "\n",
      "奇异值分解(SVD)原理与在降维中的应用 2017年1月5日 290977 125 124         奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SV     \n",
      "\n",
      "用scikit-learn进行LDA降维 2017年1月4日 42504 29 8         在线性判别分析LDA原理总结中，我们对LDA降维的原理做了总结，这里我们就对scikit-learn中LDA的降维使用做一个总结。 1.&#160;对scikit-learn中LDA类概述 在scikit-learn中， LDA类是sklearn.discriminant_analysis.LinearDis     \n",
      "\n",
      "线性判别分析LDA原理总结 2017年1月3日 300684 207 57         在主成分分析（PCA）原理总结中，我们对降维算法PCA做了总结。这里我们就对另外一种经典的降维方法线性判别分析（Linear Discriminant Analysis, 以下简称LDA）做一个总结。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了     \n",
      "\n",
      "用scikit-learn学习主成分分析(PCA) 2017年1月2日 157492 74 18         在主成分分析（PCA）原理总结中，我们对主成分分析(以下简称PCA)的原理做了总结，下面我们就总结下如何使用scikit-learn工具来进行PCA降维。 1. scikit-learn PCA类介绍 在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。     \n",
      "\n",
      "主成分分析（PCA）原理总结 2016年12月31日 238185 257 68         主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。一般我们提到降维最容易想到的算法就是PCA，下面我们就对PCA的原理做一个总结。 1. PCA的思想 PCA顾名思义，就是找出数据里最     \n",
      "\n",
      "用scikit-learn学习谱聚类 2016年12月30日 42691 47 8         在谱聚类（spectral clustering）原理总结中，我们对谱聚类的原理做了总结。这里我们就对scikit-learn中谱聚类的使用做一个总结。 1. scikit-learn谱聚类概述 在scikit-learn的类库中，sklearn.cluster.SpectralClustering     \n",
      "\n",
      "谱聚类（spectral clustering）原理总结 2016年12月29日 332987 299 88         谱聚类（spectral clustering）是广泛使用的聚类算法，比起传统的K-Means算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多，更加难能可贵的是实现起来也不复杂。在处理实际的聚类问题时，个人认为谱聚类是应该首先考虑的几种算法之一。下面我们就对谱聚类的算法     \n",
      "\n",
      "用scikit-learn学习DBSCAN聚类 2016年12月24日 128959 78 14         在DBSCAN密度聚类算法中，我们对DBSCAN聚类算法的原理做了总结，本文就对如何用scikit-learn来学习DBSCAN聚类做一个总结，重点讲述参数的意义和需要调参的参数。 1. scikit-learn中的DBSCAN类 在scikit-learn中，DBSCAN算法类为sklearn.c     \n",
      "\n",
      "DBSCAN密度聚类算法 2016年12月22日 257651 80 38         DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用     \n",
      "\n",
      "用scikit-learn学习BIRCH聚类 2016年12月19日 26095 55 5         在BIRCH聚类算法原理中，我们对BIRCH聚类算法的原理做了总结，本文就对scikit-learn中BIRCH算法的使用做一个总结。 1. scikit-learn之BIRCH类 在scikit-learn中，BIRCH类实现了原理篇里讲到的基于特征树CF Tree的聚类。因此要使用BIRCH来聚     \n",
      "\n",
      "BIRCH聚类算法原理 2016年12月14日 76552 74 25         在K-Means聚类算法原理中，我们讲到了K-Means和Mini Batch K-Means的聚类原理。这里我们再来看看另外一种常见的聚类算法BIRCH。BIRCH算法比较适合于数据量大，类别数K也比较多的情况。它运行速度很快，只需要单遍扫描数据集就能进行聚类，当然需要用到一些技巧，下面我们就对B     \n",
      "\n",
      "用scikit-learn学习K-Means聚类 2016年12月13日 119837 73 17         在K-Means聚类算法原理中，我们对K-Means的原理做了总结，本文我们就来讨论用scikit-learn来学习K-Means聚类。重点讲述如何选择合适的k值。 1. K-Means类概述 在scikit-learn中，包括两个K-Means的算法，一个是传统的K-Means算法，对应的类是KM     \n",
      "\n",
      "K-Means聚类算法原理 2016年12月12日 333763 79 41         K-Means算法是无监督的聚类算法，它实现起来比较简单，聚类效果也不错，因此应用很广泛。K-Means算法有大量的变体，本文就从最传统的K-Means算法讲起，在其基础上讲述K-Means的优化变体方法。包括初始化优化K-Means++, 距离计算优化elkan K-Means算法和大数据情况下的     \n",
      "\n",
      "scikit-learn随机森林调参小结 2016年12月11日 165601 166 34         在Bagging与随机森林算法原理小结中，我们对随机森林(Random Forest, 以下简称RF）的原理做了总结。本文就从实践的角度对RF做一个总结。重点讲述scikit-learn中RF的调参注意事项，以及和GBDT调参的异同点。 1. scikit-learn随机森林类库概述 在scikit     \n",
      "\n",
      "Bagging与随机森林算法原理小结 2016年12月10日 138210 165 35         在集成学习原理小结中，我们讲到了集成学习有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。本文就对集成学习中Bagging与随机森林算法做一个总结。 随机森林是集成学习中可以和梯度提升树GB     \n",
      "\n",
      "scikit-learn 梯度提升树(GBDT)调参小结 2016年12月9日 132672 108 26         在梯度提升树(GBDT)原理小结中，我们对GBDT的原理做了总结，本文我们就从scikit-learn里GBDT的类库使用方法作一个总结，主要会关注调参中的一些要点。 1.&#160;scikit-learn GBDT类库概述 在sacikit-learn中，GradientBoostingClassifie     \n",
      "\n",
      "梯度提升树(GBDT)原理小结 2016年12月7日 339874 617 67         在集成学习之Adaboost算法原理小结中，我们对Boosting家族的Adaboost算法做了总结，本文就对Boosting家族中另一个重要的算法梯度提升树(Gradient Boosting Decison Tree, 以下简称GBDT)做一个总结。GBDT有很多简称，有GBT（Gradient     \n",
      "\n",
      "scikit-learn Adaboost类库使用小结 2016年12月6日 100124 118 31         在集成学习之Adaboost算法原理小结中，我们对Adaboost的算法原理做了一个总结。这里我们就从实用的角度对scikit-learn中Adaboost类库的使用做一个小结，重点对调参的注意事项做一个总结。 1. Adaboost类库概述 scikit-learn中Adaboost类库比较直接，     \n",
      "\n",
      "集成学习之Adaboost算法原理小结 2016年12月5日 169381 362 70         在集成学习原理小结中，我们讲到了集成学习按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，另一类是个体学习器之间不存在强依赖关系。前者的代表算法就是是boosting系列算法。在boosting系列算法中， Adaboost是最著名的算法之一。Adaboost既可     \n",
      "\n",
      "集成学习原理小结 2016年12月4日 102927 55 67         集成学习(ensemble learning)可以说是现在非常火爆的机器学习方法了。它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。也就是我们常说的“博采众长”。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等等，可以说所有的机器学习领域都     \n",
      "\n",
      "支持向量机高斯核调参小结 2016年12月2日 42348 60 18         在支持向量机(以下简称SVM)的核函数中，高斯核(以下简称RBF)是最常用的，从理论上讲， RBF一定不比线性核函数差，但是在实际应用中，却面临着几个重要的超参数的调优问题。如果调的不好，可能比线性核函数还要差。所以我们实际应用中，能用线性核函数得到较好效果的都会选择线性核函数。如果线性核不好，我们     \n",
      "\n",
      "scikit-learn 支持向量机算法库使用小结 2016年11月30日 47796 40 12         之前通过一个系列对支持向量机(以下简称SVM)算法的原理做了一个总结，本文从实践的角度对scikit-learn SVM算法库的使用做一个小结。scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分。 1.&#160;scikit-learn SVM     \n",
      "\n",
      "支持向量机原理(五)线性支持回归 2016年11月29日 42997 41 11         支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在前四篇里面我们讲到了SVM的线性分类和非线性分类，以及在分类时用到的算法。这些都关注     \n",
      "\n",
      "支持向量机原理(三)线性不可分支持向量机与核函数 2016年11月26日 37298 43 14         支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在前面两篇我们讲到了线性可分SVM的硬间隔最大化和软间隔最大化的算法，它们对线性可分的     \n",
      "\n",
      "支持向量机原理(二) 线性支持向量机的软间隔最大化模型 2016年11月25日 44657 73 20         支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在支持向量机原理(一) 线性支持向量机中，我们对线性可分SVM的模型和损失函数优化做了     \n",
      "\n",
      "支持向量机原理(一) 线性支持向量机 2016年11月24日 118281 124 47         支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 支持向量机(Support Vecor Machine,以下简称SVM)虽然诞生只有短     \n",
      "\n",
      "最大熵模型原理小结 2016年11月23日 52585 100 15         最大熵模型(maximum entropy model， MaxEnt)也是很典型的分类算法了，它和逻辑回归类似，都是属于对数线性分类模型。在损失函数优化的过程中，使用了和支持向量机类似的凸优化技术。而对熵的使用，让我们想起了决策树算法中的ID3和C4.5算法。理解了最大熵模型，对逻辑回归，支持向量     \n",
      "\n",
      "scikit-learn 朴素贝叶斯类库使用小结 2016年11月17日 57486 42 22         之前在朴素贝叶斯算法原理小结这篇文章中，对朴素贝叶斯分类算法的原理做了一个总结。这里我们就从实战的角度来看朴素贝叶斯类库。重点讲述scikit-learn 朴素贝叶斯类库的使用要点和参数选择。 1. scikit-learn 朴素贝叶斯类库概述 朴素贝叶斯是一类比较简单的算法，scikit-lear     \n",
      "\n",
      "朴素贝叶斯算法原理小结 2016年11月16日 131365 130 48         在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数$Y=f(X)$,要么是条件分布$P(Y|X)$。但是朴素贝叶斯却是生成方法，也就是直     \n",
      "\n",
      "scikit-learn K近邻法类库使用小结 2016年11月15日 39021 13 10         在K近邻法(KNN)原理小结这篇文章，我们讨论了KNN的原理和优缺点，这里我们就从实践出发，对scikit-learn 中KNN相关的类库使用做一个小结。主要关注于类库调参时的一个经验总结。 1.&#160;scikit-learn 中KNN相关的类库概述 在scikit-learn 中，与近邻法这一大类相关     \n",
      "\n",
      "K近邻法(KNN)原理小结 2016年11月14日 87661 87 28         K近邻法(k-nearest neighbors,KNN)是一种很基本的机器学习方法了，在我们平常的生活中也会不自主的应用。比如，我们判断一个人的人品，只需要观察他来往最密切的几个人的人品好坏就可以得出了。这里就运用了KNN的思想。KNN方法既可以做分类，也可以做回归，这点和决策树算法相同。 KNN     \n",
      "\n",
      "scikit-learn决策树算法类库使用小结 2016年11月12日 149050 137 30         之前对决策树的算法原理做了总结，包括决策树算法原理(上)和决策树算法原理(下)。今天就从实践的角度来介绍决策树算法，主要是讲解使用scikit-learn来跑决策树算法，结果的可视化以及一些参数调参的关键点。 1.&#160;scikit-learn决策树算法类库介绍 scikit-learn决策树算法类库内     \n",
      "\n",
      "决策树算法原理(下) 2016年11月11日 130341 342 51         在决策树算法原理(上)这篇里，我们讲到了决策树里ID3算法，和ID3算法的改进版C4.5算法。对于C4.5算法，我们也提到了它的不足，比如模型是用较为复杂的熵来度量，使用了相对较为复杂的多叉树，只能处理分类不能处理回归等。对于这些问题， CART算法大部分做了改进。CART算法也就是我们下面的重点了     \n",
      "\n",
      "决策树算法原理(上) 2016年11月10日 136196 144 50         决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。本文就对决策树算法原理做一个总结，上篇对ID3， C4.5的算法思想做了总结，下篇重点对CART算法做一个详细的介绍。选择CART做重点介绍的原因是scikit-learn     \n",
      "\n",
      "机器学习算法的随机数据生成 2016年11月9日 35498 11 20         在学习机器学习算法的过程中，我们经常需要数据来验证算法，调试参数。但是找到一组十分合适某种特定算法类型的数据样本却不那么容易。还好numpy, scikit-learn都提供了随机数据生成的功能，我们可以自己生成适合某一种模型的数据，用随机数据来做清洗，归一化，转换，然后选择模型与算法做拟合和预测。     \n",
      "\n",
      "感知机原理小结 2016年11月8日 59224 115 40         感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。这里对感知机的原理做一个小结。      \n",
      "\n",
      "日志和告警数据挖掘经验谈 2016年11月7日 18502 17 5         最近参与了了一个日志和告警的数据挖掘项目，里面用到的一些思路在这里和大家做一个分享。 项目的需求是收集的客户系统一个月300G左右的的日志和告警数据做一个整理，主要是归类(Grouping)和关联(Correlation)，从而得到告警和日志的一些统计关系，这些统计结果可以给一线支持人员参考。 得到     \n",
      "\n",
      "scikit-learn 逻辑回归类库使用小结 2016年11月6日 51689 66 17         之前在逻辑回归原理小结这篇文章中，对逻辑回归的原理做了小结。这里接着对scikit-learn中逻辑回归类库的我的使用经验做一个总结。重点讲述调参中要注意的事项。 1. 概述 在scikit-learn中，与逻辑回归有关的主要是这3个类。LogisticRegression， LogisticReg     \n",
      "\n",
      "逻辑回归原理小结 2016年11月4日 109139 199 41         逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。虽然它名字里面有“回归”两个字，却不是一个回归算法。那为什么有“回归”这个误导性的词呢？个人认为，虽然逻辑回归是分类模型，但是它的原理里面却残留着回归模型的影子，本文对逻辑回归原理做一个总结。 1. 从线性回归到逻辑回归 我们知道，线性回归的模     \n",
      "\n",
      "scikit-learn 线性回归算法库小结 2016年11月3日 36290 27 13         scikit-learn对于线性回归提供了比较多的类库，这些类库都可以用来做线性回归分析，本文就对这些类库的使用做一个总结，重点讲述这些线性回归算法库的不同和各自的使用场景。 线性回归的目的是要得到输出向量\\(\\mathbf{Y}\\)和输入特征\\(\\mathbf{X}\\)之间的线性关系，求出线性回归     \n",
      "\n",
      "用scikit-learn和pandas学习Ridge回归 2016年11月2日 30674 44 11         本文将用一个例子来讲述怎么用scikit-learn和pandas来学习Ridge回归。 1. Ridge回归的损失函数 在我的另外一遍讲线性回归的文章中，对Ridge回归做了一些介绍，以及什么时候适合用 Ridge回归。如果对什么是Ridge回归还完全不清楚的建议阅读我这篇文章。 线性回归原理小结     \n",
      "\n",
      "Lasso回归算法： 坐标轴下降法与最小角回归法小结 2016年11月1日 71522 73 24         前面的文章对线性回归做了一个小结，文章在这：&#160;线性回归原理小结。里面对线程回归的正则化也做了一个初步的介绍。提到了线程回归的L2正则化-Ridge回归，以及线程回归的L1正则化-Lasso回归。但是对于Lasso回归的解法没有提及，本文是对该文的补充和扩展。以下都用矩阵法表示，如果对于矩阵分析不熟悉     \n",
      "\n",
      "用scikit-learn和pandas学习线性回归 2016年10月31日 105032 77 21         对于想深入了解线性回归的童鞋，这里给出一个完整的例子，详细学完这个例子，对用scikit-learn来运行线性回归，评估模型不会有什么问题了。 1. 获取数据，定义问题 没有数据，当然没法研究机器学习啦。:) 这里我们用UCI大学公开的机器学习数据来跑线性回归。 数据的介绍在这：&#160;http://ar     \n",
      "\n",
      "scikit-learn 和pandas 基于windows单机机器学习环境的搭建 2016年10月30日 17622 8 4         很多朋友想学习机器学习，却苦于环境的搭建，这里给出windows上scikit-learn研究开发环境的搭建步骤。 Step 1. Python的安装 python有2.x和3.x的版本之分，但是很多好的机器学习python库都不支持3.x，因此，推荐安装2.7版本的python。当前最新的pyth     \n",
      "\n",
      "机器学习研究与开发平台的选择 2016年10月28日 24037 30 15         目前机器学习可以说是百花齐放阶段，不过如果要学习或者研究机器学习，进而用到生产环境，对平台，开发语言，机器学习库的选择就要费一番脑筋了。这里就我自己的机器学习经验做一个建议，仅供参考。 首先，对于平台选择的第一个问题是，你是要用于生产环境，也就是具体的产品中,还是仅仅是做研究学习用？ 1. 生产环境     \n",
      "\n",
      "线性回归原理小结 2016年10月28日 60593 111 24         线性回归可以说是机器学习中最基本的问题类型了，这里就对线性回归的原理和算法做一个小结。 1. 线性回归的模型函数和损失函数 线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应于n维特征和一个结果输出，如下： \\((x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_     \n",
      "\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 42004 35 16         在机器学习的算法评估中，尤其是分类算法评估中，我们经常听到精确率(precision)与召回率(recall)，RoC曲线与PR曲线这些概念，那这些概念到底有什么用处呢？ 首先，我们需要搞清楚几个拗口的概念： 1. TP, FP, TN, FN 听起来还是很费劲，不过我们用一张图就很容易理解了。图如     \n",
      "\n",
      "最小二乘法小结 2016年10月19日 108285 72 58         最小二乘法是用来做函数拟合或者求函数极值的方法。在机器学习，尤其是回归模型中，经常可以看到最小二乘法的身影，这里就对我对最小二乘法的认知做一个小结。 1.最小二乘法的原理与要解决的问题 最小二乘法是由勒让德在19世纪发现的，原理的一般形式很简单，当然发现的过程是非常艰难的。形式如下式：$$目标函数      \n",
      "\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 542732 247 165         在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。 1. 梯度 在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数     \n",
      "\n",
      "文章数量：133\n"
     ]
    }
   ],
   "source": [
    "# 初始化文章数量\n",
    "count = 0\n",
    "\n",
    "for url in urls:\n",
    "    # 发送 GET 请求，获取网页内容\n",
    "    res = requests.get(url)\n",
    "    res.encoding = 'utf-8'\n",
    "    # (.*?)无法匹配换行符，所以要先去除换行符\n",
    "    str_text = res.text.replace(\"\\n\", \"\")\n",
    "\n",
    "    # 保存原始 HTML 内容\n",
    "    # 因有反爬功能存在，返回的内容和我们看到的内容可能不一致，需要保存下来看一下\n",
    "    with open('./output/cnblogs.html', \"w\", encoding='utf-8') as f:\n",
    "        f.write(str_text)\n",
    "\n",
    "    # 提取博客条目\n",
    "    # 开始部分：<div class=\"day\"\n",
    "    # 提取部分：(.*?)\n",
    "    # 结尾部分：<div class=\"clear\"></div></div>\n",
    "    blogs = re.findall(r'<div class=\"day\"(.*?)<div class=\"clear\"></div></div>', str_text)\n",
    "    # print(len(blogs), blogs)\n",
    "    # break\n",
    "    \n",
    "    # 解析每个博客的详细信息\n",
    "    for blog in blogs:\n",
    "        # 标题，时间，阅读量，评论数，推荐数，摘要\n",
    "        title = re.findall(r'<span>(.*?)</span>', blog)[0].strip()\n",
    "        date = re.findall(r'<div class=\"dayTitle\">.*?>(.*?)</a>', blog)[0]  # .*?>：匹配 <a href=\"...\">，包括前面的空格\n",
    "        read = re.findall(r'阅读\\((.*?)\\)', blog)[0]\n",
    "        comment = re.findall(r'评论\\((.*?)\\)', blog)[0]\n",
    "        recommendation = re.findall(r'推荐\\((.*?)\\)', blog)[0]\n",
    "        abstract = re.findall(r'摘要：(.*?)<a', blog)[0]\n",
    "        print(title, date, read, comment, recommendation, abstract, \"\\n\")\n",
    "\n",
    "        # 连接成一个字符串\n",
    "        strs = f\"{title}\\t{date}\\t{read}\\t{comment}\\t{recommendation}\\t{abstract}\"\n",
    "        # print(strs)\n",
    "        \n",
    "        # 以追加模式将字符串写入文件，每行一个博客记录\n",
    "        with open('./output/cnblogs_datas.txt', 'a', encoding='utf-8') as f:\n",
    "            f.write(strs + '\\n')\n",
    "        count += 1\n",
    "        \n",
    "print(f\"文章数量：{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d3cc9-18bd-4524-baf3-1a41f9489795",
   "metadata": {},
   "source": [
    "## 方法二：Xpath\n",
    "\n",
    "1、复制 Xpath 路径：\n",
    "\n",
    "![](./images/cnblogs01.png)\n",
    "\n",
    "2、按 Ctrl + F，打开搜索窗口，粘贴 Xpath 路径：\n",
    "\n",
    "<img src=\"./images/cnblogs02.png\" width=800>\n",
    "\n",
    "3、在搜索输入框，从右向左删 Xpath 路径，直到出现文章标题；<br>\n",
    "点击 ↑↓ 按钮，可以选择文章标题。\n",
    "\n",
    "NOTE: 第 1 个和最后 2 个 div 没有标题，中间 10 个（class=\"day\"）有标题，需要在程序中做判断。\n",
    "\n",
    "<img src=\"./images/cnblogs03.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b217c0e1-8eb6-4759-a3a1-8dd1a97b753f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost类库使用小结 2019年7月1日 阅读(66627) 评论(135) 推荐(21)\n",
      "XGBoost算法原理小结 2019年6月5日 阅读(78411) 评论(201) 推荐(36)\n",
      "机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导 2019年5月27日 阅读(43132) 评论(27) 推荐(7)\n",
      "机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则 2019年5月7日 阅读(57167) 评论(71) 推荐(29)\n",
      "机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法 2019年4月29日 阅读(41449) 评论(84) 推荐(19)\n",
      "机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法 2019年4月26日 阅读(43034) 评论(46) 推荐(22)\n",
      "机器学习中的矩阵向量求导(一) 求导定义与求导布局 2019年4月22日 阅读(67509) 评论(19) 推荐(55)\n",
      "强化学习(十九) AlphaGo Zero强化学习原理 2019年3月27日 阅读(39821) 评论(69) 推荐(14)\n",
      "强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS) 2019年3月4日 阅读(49710) 评论(29) 推荐(5)\n",
      "强化学习(十七) 基于模型的强化学习与Dyna算法框架 2019年2月15日 阅读(25460) 评论(26) 推荐(2)\n",
      "强化学习(十六) 深度确定性策略梯度(DDPG) 2019年2月1日 阅读(123501) 评论(318) 推荐(24)\n",
      "强化学习(十五) A3C 2019年1月29日 阅读(71528) 评论(144) 推荐(4)\n",
      "强化学习(十四) Actor-Critic 2019年1月15日 阅读(114927) 评论(148) 推荐(9)\n",
      "强化学习(十三) 策略梯度(Policy Gradient) 2018年12月18日 阅读(122592) 评论(177) 推荐(14)\n",
      "强化学习(十二) Dueling DQN 2018年11月8日 阅读(58435) 评论(74) 推荐(5)\n",
      "强化学习(十一) Prioritized Replay DQN 2018年10月16日 阅读(52696) 评论(153) 推荐(14)\n",
      "强化学习（十）Double DQN (DDQN) 2018年10月12日 阅读(108696) 评论(77) 推荐(8)\n",
      "强化学习（九）Deep Q-Learning进阶之Nature DQN 2018年10月8日 阅读(69415) 评论(84) 推荐(12)\n",
      "强化学习（八）价值函数的近似表示与Deep Q-Learning 2018年9月28日 阅读(89548) 评论(203) 推荐(13)\n",
      "强化学习（七）时序差分离线控制算法Q-Learning 2018年9月19日 阅读(61517) 评论(109) 推荐(15)\n",
      "强化学习（六）时序差分在线控制算法SARSA 2018年9月9日 阅读(60728) 评论(87) 推荐(10)\n",
      "强化学习（五）用时序差分法（TD）求解 2018年8月24日 阅读(77616) 评论(131) 推荐(16)\n",
      "强化学习（四）用蒙特卡罗法（MC）求解 2018年8月17日 阅读(73175) 评论(108) 推荐(18)\n",
      "强化学习（三）用动态规划（DP）求解 2018年8月12日 阅读(74303) 评论(103) 推荐(22)\n",
      "强化学习（二）马尔科夫决策过程(MDP) 2018年8月5日 阅读(159034) 评论(142) 推荐(26)\n",
      "强化学习（一）模型基础 2018年7月29日 阅读(153998) 评论(75) 推荐(36)\n",
      "异常点检测算法小结 2018年7月15日 阅读(52156) 评论(78) 推荐(11)\n",
      "tensorflow机器学习模型的跨平台上线 2018年7月1日 阅读(14256) 评论(18) 推荐(2)\n",
      "用PMML实现机器学习模型的跨平台上线 2018年6月24日 阅读(43199) 评论(79) 推荐(15)\n",
      "用tensorflow学习贝叶斯个性化排序(BPR) 2018年6月10日 阅读(21476) 评论(46) 推荐(5)\n",
      "贝叶斯个性化排序(BPR)算法小结 2018年6月3日 阅读(48331) 评论(68) 推荐(16)\n",
      "特征工程之特征预处理 2018年5月26日 阅读(31583) 评论(113) 推荐(27)\n",
      "特征工程之特征表达 2018年5月19日 阅读(28253) 评论(107) 推荐(12)\n",
      "特征工程之特征选择 2018年5月13日 阅读(57252) 评论(115) 推荐(31)\n",
      "用gensim学习word2vec 2017年8月3日 阅读(95553) 评论(104) 推荐(24)\n",
      "word2vec原理(三) 基于Negative Sampling的模型 2017年7月28日 阅读(105869) 评论(131) 推荐(27)\n",
      "word2vec原理(二) 基于Hierarchical Softmax的模型 2017年7月27日 阅读(137786) 评论(290) 推荐(45)\n",
      "word2vec原理(一) CBOW与Skip-Gram模型基础 2017年7月13日 阅读(254233) 评论(112) 推荐(44)\n",
      "条件随机场CRF(三) 模型学习与维特比算法解码 2017年6月23日 阅读(28614) 评论(66) 推荐(7)\n",
      "条件随机场CRF(二) 前向后向算法评估标记序列概率 2017年6月22日 阅读(23180) 评论(80) 推荐(5)\n",
      "条件随机场CRF(一)从随机场到线性链条件随机场 2017年6月19日 阅读(54902) 评论(99) 推荐(21)\n",
      "用hmmlearn学习隐马尔科夫模型HMM 2017年6月13日 阅读(54977) 评论(190) 推荐(15)\n",
      "隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 2017年6月12日 阅读(43414) 评论(35) 推荐(8)\n",
      "隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 2017年6月10日 阅读(40495) 评论(97) 推荐(12)\n",
      "隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 2017年6月8日 阅读(63219) 评论(61) 推荐(20)\n",
      "隐马尔科夫模型HMM（一）HMM模型 2017年6月6日 阅读(119704) 评论(37) 推荐(35)\n",
      "EM算法原理总结 2017年5月27日 阅读(103419) 评论(131) 推荐(33)\n",
      "用scikit-learn学习LDA主题模型 2017年5月26日 阅读(67781) 评论(115) 推荐(13)\n",
      "文本主题模型之LDA(三) LDA求解之变分推断EM算法 2017年5月22日 阅读(32770) 评论(101) 推荐(11)\n",
      "文本主题模型之LDA(二) LDA求解之Gibbs采样算法 2017年5月18日 阅读(55342) 评论(218) 推荐(10)\n",
      "文本主题模型之LDA(一) LDA基础 2017年5月17日 阅读(214115) 评论(105) 推荐(29)\n",
      "文本主题模型之非负矩阵分解(NMF) 2017年5月5日 阅读(31284) 评论(15) 推荐(9)\n",
      "文本主题模型之潜在语义索引(LSI) 2017年5月4日 阅读(40489) 评论(48) 推荐(11)\n",
      "英文文本挖掘预处理流程总结 2017年4月24日 阅读(27689) 评论(16) 推荐(9)\n",
      "中文文本挖掘预处理流程总结 2017年4月21日 阅读(59109) 评论(76) 推荐(20)\n",
      "文本挖掘预处理之TF-IDF 2017年4月11日 阅读(77670) 评论(39) 推荐(20)\n",
      "文本挖掘预处理之向量化与Hash Trick 2017年4月10日 阅读(28224) 评论(36) 推荐(8)\n",
      "文本挖掘的分词原理 2017年4月7日 阅读(37934) 评论(65) 推荐(17)\n",
      "MCMC(四)Gibbs采样 2017年3月30日 阅读(101877) 评论(145) 推荐(30)\n",
      "MCMC(三)MCMC采样和M-H采样 2017年3月29日 阅读(122198) 评论(267) 推荐(34)\n",
      "MCMC(二)马尔科夫链 2017年3月28日 阅读(120423) 评论(150) 推荐(49)\n",
      "MCMC(一)蒙特卡罗方法 2017年3月27日 阅读(181426) 评论(116) 推荐(69)\n",
      "受限玻尔兹曼机（RBM）原理总结 2017年3月11日 阅读(48466) 评论(44) 推荐(17)\n",
      "LSTM模型与前向反向传播算法 2017年3月8日 阅读(95172) 评论(173) 推荐(34)\n",
      "循环神经网络(RNN)模型与前向反向传播算法 2017年3月6日 阅读(156117) 评论(217) 推荐(28)\n",
      "卷积神经网络(CNN)反向传播算法 2017年3月3日 阅读(204289) 评论(256) 推荐(61)\n",
      "卷积神经网络(CNN)前向传播算法 2017年3月2日 阅读(70467) 评论(56) 推荐(17)\n",
      "卷积神经网络(CNN)模型结构 2017年3月1日 阅读(211701) 评论(79) 推荐(32)\n",
      "深度神经网络（DNN）的正则化 2017年2月27日 阅读(40948) 评论(40) 推荐(13)\n",
      "深度神经网络（DNN）损失函数和激活函数的选择 2017年2月24日 阅读(87075) 评论(164) 推荐(15)\n",
      "深度神经网络（DNN）反向传播算法(BP) 2017年2月21日 阅读(132772) 评论(176) 推荐(40)\n",
      "深度神经网络（DNN）模型与前向传播算法 2017年2月20日 阅读(222483) 评论(47) 推荐(52)\n",
      "分解机(Factorization Machines)推荐算法原理 2017年2月6日 阅读(51981) 评论(72) 推荐(11)\n",
      "用Spark学习矩阵分解推荐算法 2017年2月4日 阅读(21894) 评论(44) 推荐(6)\n",
      "SimRank协同过滤推荐算法 2017年2月3日 阅读(19971) 评论(36) 推荐(5)\n",
      "矩阵分解在协同过滤推荐算法中的应用 2017年1月26日 阅读(48077) 评论(86) 推荐(19)\n",
      "协同过滤推荐算法总结 2017年1月25日 阅读(95413) 评论(66) 推荐(41)\n",
      "用Spark学习FP Tree算法和PrefixSpan算法 2017年1月22日 阅读(14262) 评论(24) 推荐(4)\n",
      "PrefixSpan算法原理总结 2017年1月20日 阅读(39662) 评论(32) 推荐(8)\n",
      "FP Tree算法原理总结 2017年1月19日 阅读(84631) 评论(80) 推荐(48)\n",
      "Apriori算法原理总结 2017年1月17日 阅读(123648) 评论(52) 推荐(31)\n",
      "典型关联分析(CCA)原理总结 2017年1月16日 阅读(69993) 评论(59) 推荐(20)\n",
      "用scikit-learn研究局部线性嵌入(LLE) 2017年1月11日 阅读(11553) 评论(0) 推荐(5)\n",
      "局部线性嵌入(LLE)原理总结 2017年1月10日 阅读(68070) 评论(94) 推荐(15)\n",
      "奇异值分解(SVD)原理与在降维中的应用 2017年1月5日 阅读(290977) 评论(125) 推荐(124)\n",
      "用scikit-learn进行LDA降维 2017年1月4日 阅读(42504) 评论(29) 推荐(8)\n",
      "线性判别分析LDA原理总结 2017年1月3日 阅读(300684) 评论(207) 推荐(57)\n",
      "用scikit-learn学习主成分分析(PCA) 2017年1月2日 阅读(157492) 评论(74) 推荐(18)\n",
      "主成分分析（PCA）原理总结 2016年12月31日 阅读(238185) 评论(257) 推荐(68)\n",
      "用scikit-learn学习谱聚类 2016年12月30日 阅读(42691) 评论(47) 推荐(8)\n",
      "谱聚类（spectral clustering）原理总结 2016年12月29日 阅读(332987) 评论(299) 推荐(88)\n",
      "用scikit-learn学习DBSCAN聚类 2016年12月24日 阅读(128959) 评论(78) 推荐(14)\n",
      "DBSCAN密度聚类算法 2016年12月22日 阅读(257651) 评论(80) 推荐(38)\n",
      "用scikit-learn学习BIRCH聚类 2016年12月19日 阅读(26095) 评论(55) 推荐(5)\n",
      "BIRCH聚类算法原理 2016年12月14日 阅读(76552) 评论(74) 推荐(25)\n",
      "用scikit-learn学习K-Means聚类 2016年12月13日 阅读(119837) 评论(73) 推荐(17)\n",
      "K-Means聚类算法原理 2016年12月12日 阅读(333763) 评论(79) 推荐(41)\n",
      "scikit-learn随机森林调参小结 2016年12月11日 阅读(165601) 评论(166) 推荐(34)\n",
      "Bagging与随机森林算法原理小结 2016年12月10日 阅读(138210) 评论(165) 推荐(35)\n",
      "scikit-learn 梯度提升树(GBDT)调参小结 2016年12月9日 阅读(132672) 评论(108) 推荐(26)\n",
      "梯度提升树(GBDT)原理小结 2016年12月7日 阅读(339874) 评论(617) 推荐(67)\n",
      "scikit-learn Adaboost类库使用小结 2016年12月6日 阅读(100124) 评论(118) 推荐(31)\n",
      "集成学习之Adaboost算法原理小结 2016年12月5日 阅读(169381) 评论(362) 推荐(70)\n",
      "集成学习原理小结 2016年12月4日 阅读(102927) 评论(55) 推荐(67)\n",
      "支持向量机高斯核调参小结 2016年12月2日 阅读(42348) 评论(60) 推荐(18)\n",
      "scikit-learn 支持向量机算法库使用小结 2016年11月30日 阅读(47796) 评论(40) 推荐(12)\n",
      "支持向量机原理(五)线性支持回归 2016年11月29日 阅读(42997) 评论(41) 推荐(11)\n",
      "支持向量机原理(三)线性不可分支持向量机与核函数 2016年11月26日 阅读(37298) 评论(43) 推荐(14)\n",
      "支持向量机原理(二) 线性支持向量机的软间隔最大化模型 2016年11月25日 阅读(44657) 评论(73) 推荐(20)\n",
      "支持向量机原理(一) 线性支持向量机 2016年11月24日 阅读(118281) 评论(124) 推荐(47)\n",
      "最大熵模型原理小结 2016年11月23日 阅读(52585) 评论(100) 推荐(15)\n",
      "scikit-learn 朴素贝叶斯类库使用小结 2016年11月17日 阅读(57486) 评论(42) 推荐(22)\n",
      "朴素贝叶斯算法原理小结 2016年11月16日 阅读(131365) 评论(130) 推荐(48)\n",
      "scikit-learn K近邻法类库使用小结 2016年11月15日 阅读(39021) 评论(13) 推荐(10)\n",
      "K近邻法(KNN)原理小结 2016年11月14日 阅读(87661) 评论(87) 推荐(28)\n",
      "scikit-learn决策树算法类库使用小结 2016年11月12日 阅读(149050) 评论(137) 推荐(30)\n",
      "决策树算法原理(下) 2016年11月11日 阅读(130341) 评论(342) 推荐(51)\n",
      "决策树算法原理(上) 2016年11月10日 阅读(136196) 评论(144) 推荐(50)\n",
      "机器学习算法的随机数据生成 2016年11月9日 阅读(35498) 评论(11) 推荐(20)\n",
      "感知机原理小结 2016年11月8日 阅读(59224) 评论(115) 推荐(40)\n",
      "日志和告警数据挖掘经验谈 2016年11月7日 阅读(18502) 评论(17) 推荐(5)\n",
      "scikit-learn 逻辑回归类库使用小结 2016年11月6日 阅读(51689) 评论(66) 推荐(17)\n",
      "逻辑回归原理小结 2016年11月4日 阅读(109139) 评论(199) 推荐(41)\n",
      "scikit-learn 线性回归算法库小结 2016年11月3日 阅读(36290) 评论(27) 推荐(13)\n",
      "用scikit-learn和pandas学习Ridge回归 2016年11月2日 阅读(30674) 评论(44) 推荐(11)\n",
      "Lasso回归算法： 坐标轴下降法与最小角回归法小结 2016年11月1日 阅读(71522) 评论(73) 推荐(24)\n",
      "用scikit-learn和pandas学习线性回归 2016年10月31日 阅读(105032) 评论(77) 推荐(21)\n",
      "scikit-learn 和pandas 基于windows单机机器学习环境的搭建 2016年10月30日 阅读(17622) 评论(8) 推荐(4)\n",
      "机器学习研究与开发平台的选择 2016年10月28日 阅读(24037) 评论(30) 推荐(15)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    # 发送 GET 请求，获取 HTML 网页内容\n",
    "    res = requests.get(url)\n",
    "    # 将 HTML 文本解析成可进行 XPath 查询的树结构对象\n",
    "    xpath_html = etree.HTML(res.text)\n",
    "    # print(xpath_html)  # <Element html at 0x1fbd379f680>\n",
    "    \n",
    "    # 提取博客文章块（返回列表）\n",
    "    # *：匹配任何元素节点\n",
    "    # @：选取属性\n",
    "    # //*[@id=\"mainContent\"]：查找 id 为 mainContent 的任何元素\n",
    "    blogs = xpath_html.xpath('//*[@id=\"mainContent\"]/div/div')\n",
    "                            # //*[@id=\"mainContent\"]/div/div[2]/div[2]/a/span  # 第 1 篇博文的 xpath\n",
    "    # print(len(blogs), blogs)\n",
    "    \"\"\"\n",
    "    13 [<Element div at 0x1fbd3760e80>, <Element div at 0x1fbd3760180>, <Element div at 0x1fbd3763f40>, \n",
    "    <Element div at 0x1fbd2e0fd40>, <Element div at 0x1fbd37b7740>, <Element div at 0x1fbd37b7e00>, \n",
    "    <Element div at 0x1fbd37b6500>, <Element div at 0x1fbd37b5740>, <Element div at 0x1fbd37b5080>, \n",
    "    <Element div at 0x1fbd37b4800>, <Element div at 0x1fbd37b4140>, <Element div at 0x1fbd37b5800>, \n",
    "    <Element div at 0x1fbd37b5f80>]\n",
    "    \"\"\"\n",
    "    # break\n",
    "\n",
    "    for blog in blogs:\n",
    "        # 标题，时间，阅读量，评论数，推荐数\n",
    "        # 通过查看 HTML 源代码定位\n",
    "        title = blog.xpath('./div[2]/a/span/text()')  # 第 2 个 <div> 下的 <a> 的 <span> 的文本内容\n",
    "        date =  blog.xpath('./div[1]/a/text()')      \n",
    "        read = blog.xpath('./div[5]/span[1]/text()')\n",
    "        comment = blog.xpath('./div[5]/span[2]/text()')\n",
    "        recommendation = blog.xpath('./div[5]/span[3]/text()')\n",
    "        # 如果 title 非空（成功提取到标题）\n",
    "        if title:\n",
    "            # 去掉标题两边的换行符和空格\n",
    "            title = title[0].strip()\n",
    "            date = date[0].strip()\n",
    "            read = read[0]\n",
    "            comment = comment[0]\n",
    "            recommendation = recommendation[0]\n",
    "            print(title, date, read, comment, recommendation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb89080-701a-4ed0-9106-a219d8fea09f",
   "metadata": {},
   "source": [
    "## 方法三：bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f094bf-7771-4ee7-9149-ebecafdb0d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n",
      "线性回归原理小结 2016年10月28日 阅读(60593) 评论(111) 推荐(24)\n",
      "精确率与召回率，RoC曲线与PR曲线 2016年10月24日 阅读(42004) 评论(35) 推荐(16)\n",
      "最小二乘法小结 2016年10月19日 阅读(108285) 评论(72) 推荐(58)\n",
      "梯度下降（Gradient Descent）小结 2016年10月17日 阅读(542732) 评论(247) 推荐(165)\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    # class可以有多个，id 是唯一的\n",
    "    # '.'：class 属性\n",
    "    # 空格代表子节点\n",
    "    # '#'：id 属性\n",
    "    # 使用 BeautifulSoup 库来解析 HTML 内容，用的是 lxml 解析器\n",
    "    bs = BeautifulSoup(res.text, 'lxml')\n",
    "    blogs = bs.select('.day')\n",
    "    for blog in blogs:\n",
    "        title = blog.select('.postTitle span')[0].string.strip()\n",
    "        date =  blog.select('.dayTitle a')[0].string.strip()\n",
    "        read = blog.select('.post-view-count')[0].string\n",
    "        comment = blog.select('.post-comment-count')[0].string\n",
    "        recommendation = blog.select('.post-digg-count')[0].string\n",
    "        if title:\n",
    "            print(title, date, read, comment, recommendation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fb13059-56b1-42eb-8722-2121d848a2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'b']\n",
      "['ccbbcsdabcb']\n",
      "['c', 'b']\n",
      "['', 'b']\n"
     ]
    }
   ],
   "source": [
    "# ?在正则表达式中有两层含义\n",
    "# 如果 ? 出现在代码次数的规则后面则代表非贪婪模式，否则代表 0 或 1 次（贪婪模式）\n",
    "strs = 'accbbcsdabcbc'\n",
    "print(re.findall(r'a(.*?)c', strs))\n",
    "print(re.findall(r'a(.*)c', strs))\n",
    "print(re.findall(r'a(.?)c', strs))   # ['c', 'b']\n",
    "print(re.findall(r'a(.??)c', strs))  # ['', 'b']  # 在这里第一个问号是代表 0 或 1 次，第二个问号代表非贪婪模式"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
