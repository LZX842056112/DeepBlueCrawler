XGBoost类库使用小结	2019年7月1日	66952	135	21	        在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth    
XGBoost算法原理小结	2019年6月5日	78728	201	36	        在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P    
机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导	2019年5月27日	43324	27	7	        在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章    
机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则	2019年5月7日	57439	71	29	        在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果    
机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法	2019年4月29日	41657	84	19	        在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量    
机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法	2019年4月26日	43307	46	22	        在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导    
机器学习中的矩阵向量求导(一) 求导定义与求导布局	2019年4月22日	67913	19	55	        在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的    
强化学习(十九) AlphaGo Zero强化学习原理	2019年3月27日	40187	69	14	        在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG    
强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)	2019年3月4日	50044	29	5	        在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参    
强化学习(十七) 基于模型的强化学习与Dyna算法框架	2019年2月15日	25644	26	2	        在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy    
XGBoost类库使用小结	2019年7月1日	66979	135	21	        在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth    
XGBoost算法原理小结	2019年6月5日	78759	201	36	        在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P    
机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导	2019年5月27日	43338	27	7	        在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章    
机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则	2019年5月7日	57463	71	29	        在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果    
机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法	2019年4月29日	41674	84	19	        在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量    
机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法	2019年4月26日	43322	46	22	        在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导    
机器学习中的矩阵向量求导(一) 求导定义与求导布局	2019年4月22日	67941	19	55	        在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的    
强化学习(十九) AlphaGo Zero强化学习原理	2019年3月27日	40211	69	14	        在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG    
强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)	2019年3月4日	50054	29	5	        在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参    
强化学习(十七) 基于模型的强化学习与Dyna算法框架	2019年2月15日	25649	26	2	        在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy    
XGBoost类库使用小结	2019年7月1日	66979	135	21	        在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth    
XGBoost算法原理小结	2019年6月5日	78759	201	36	        在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P    
机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导	2019年5月27日	43338	27	7	        在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章    
机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则	2019年5月7日	57463	71	29	        在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果    
机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法	2019年4月29日	41674	84	19	        在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量    
机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法	2019年4月26日	43322	46	22	        在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导    
机器学习中的矩阵向量求导(一) 求导定义与求导布局	2019年4月22日	67941	19	55	        在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的    
强化学习(十九) AlphaGo Zero强化学习原理	2019年3月27日	40211	69	14	        在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG    
强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)	2019年3月4日	50054	29	5	        在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参    
强化学习(十七) 基于模型的强化学习与Dyna算法框架	2019年2月15日	25649	26	2	        在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy    
强化学习(十六) 深度确定性策略梯度(DDPG)	2019年2月1日	124117	318	25	        在强化学习(十五) A3C中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Poli    
强化学习(十五) A3C	2019年1月29日	71796	144	5	        在强化学习(十四) Actor-Critic中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C    
强化学习(十四) Actor-Critic	2019年1月15日	115531	148	10	        在强化学习(十三) 策略梯度(Policy Gradient)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。 在本篇我们讨论策略(Policy    
强化学习(十三) 策略梯度(Policy Gradient)	2018年12月18日	123030	177	14	        在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradien    
强化学习(十二) Dueling DQN	2018年11月8日	58665	74	5	        在强化学习(十一) Prioritized Replay DQN中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的deep RL tutorial和Dueling DQN的论文&lt;Dueling N    
强化学习(十一) Prioritized Replay DQN	2018年10月16日	52924	153	14	        在强化学习（十）Double DQN (DDQN)中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay     
强化学习（十）Double DQN (DDQN)	2018年10月12日	109163	77	8	        在强化学习（九）Deep Q-Learning进阶之Nature DQN中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称D    
强化学习（九）Deep Q-Learning进阶之Nature DQN	2018年10月8日	69650	84	12	        在强化学习（八）价值函数的近似表示与Deep Q-Learning中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 201    
强化学习（八）价值函数的近似表示与Deep Q-Learning	2018年9月28日	89834	203	14	        在强化学习系列的前七篇里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。 Deep&#160;Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。 1. 为何需要价值函数    
强化学习（七）时序差分离线控制算法Q-Learning	2018年9月19日	61739	109	15	        在强化学习（六）时序差分在线控制算法SARSA中我们讨论了时序差分的在线控制算法SARSA，而另一类时序差分的离线控制算法还没有讨论，因此本文我们关注于时序差分离线控制算法，主要是经典的Q-Learning算法。 Q-Learning这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部    
强化学习（六）时序差分在线控制算法SARSA	2018年9月9日	60953	87	10	        在强化学习（五）用时序差分法（TD）求解中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。 SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。 1.&#160;SARSA算法的引入 S    
强化学习（五）用时序差分法（TD）求解	2018年8月24日	77983	131	16	        在强化学习（四）用蒙特卡罗法（MC）求解中，我们讲到了使用蒙特卡罗法来求解强化学习问题的方法，虽然蒙特卡罗法很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果我们没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。本文我们就来讨论可以不使用完整状态序列求解强化    
强化学习（四）用蒙特卡罗法（MC）求解	2018年8月17日	73502	108	18	        在强化学习（三）用动态规划（DP）求解中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型$P$都无法知道，这时动态规划法根本没法    
强化学习（三）用动态规划（DP）求解	2018年8月12日	74705	103	22	        在强化学习（二）马尔科夫决策过程(MDP)中，我们讨论了用马尔科夫假设来简化强化学习模型的复杂度，这一篇我们在马尔科夫假设和贝尔曼方程的基础上讨论使用动态规划(Dynamic Programming, DP)来求解强化学习的问题。 动态规划这一篇对应Sutton书的第四章和UCL强化学习课程的第三讲    
强化学习（二）马尔科夫决策过程(MDP)	2018年8月5日	159663	142	26	        在强化学习（一）模型基础中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。 MDP    
强化学习（一）模型基础	2018年7月29日	154851	75	36	        从今天开始整理强化学习领域的知识，主要参考的资料是Sutton的强化学习书和UCL强化学习的课程。这个系列大概准备写10到20篇，希望写完后自己的强化学习碎片化知识可以得到融会贯通，也希望可以帮到更多的人，毕竟目前系统的讲解强化学习的中文资料不太多。 第一篇会从强化学习的基本概念讲起，对应Sutto    
异常点检测算法小结	2018年7月15日	52287	78	11	        异常点检测，有时也叫离群点检测，英文一般叫做Novelty Detection或者Outlier Detection,是比较常见的一类非监督学习算法，这里就对异常点检测算法做一个总结。 1.&#160;异常点检测算法使用场景 什么时候我们需要异常点检测算法呢？常见的有三种情况。一是在做特征工程的时候需要对异常    
tensorflow机器学习模型的跨平台上线	2018年7月1日	14312	18	2	        在用PMML实现机器学习模型的跨平台上线中，我们讨论了使用PMML文件来实现跨平台模型上线的方法，这个方法当然也适用于tensorflow生成的模型，但是由于tensorflow模型往往较大，使用无法优化的PMML文件大多数时候很笨拙，因此本文我们专门讨论下tensorflow机器学习模型的跨平台上    
用PMML实现机器学习模型的跨平台上线	2018年6月24日	43263	79	15	        在机器学习用于产品的时候，我们经常会遇到跨平台的问题。比如我们用Python基于一系列的机器学习库训练了一个模型，但是有时候其他的产品和项目想把这个模型集成进去，但是这些产品很多只支持某些特定的生产环境比如Java，为了上一个机器学习模型去大动干戈修改环境配置很不划算，此时我们就可以考虑用预测模型标    
用tensorflow学习贝叶斯个性化排序(BPR)	2018年6月10日	21527	46	5	        在贝叶斯个性化排序(BPR)算法小结中，我们对贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)的原理做了讨论，本文我们将从实践的角度来使用BPR做一个简单的推荐。由于现有主流开源类库都没有BPR，同时它又比较简单，因此用tensorflow自己实现一个    
贝叶斯个性化排序(BPR)算法小结	2018年6月3日	48510	68	16	        在矩阵分解在协同过滤推荐算法中的应用中，我们讨论过像funkSVD之类的矩阵分解方法如何用于推荐。今天我们讲另一种在实际产品中用的比较多的推荐算法:贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)，它也用到了矩阵分解，但是和funkSVD家族却有很多不    
特征工程之特征预处理	2018年5月26日	31736	113	27	        在前面我们分别讨论了特征工程中的特征选择与特征表达，本文我们来讨论特征预处理的相关问题。主要包括特征的归一化和标准化，异常特征样本清洗与样本数据不平衡问题的处理。 1. 特征的标准化和归一化 由于标准化和归一化这两个词经常混用，所以本文不再区别标准化和归一化，而通过具体的标准化和归一化方法来区别具体    
特征工程之特征表达	2018年5月19日	28365	107	12	        在特征工程之特征选择中，我们讲到了特征选择的一些要点。本篇我们继续讨论特征工程，不过会重点关注于特征表达部分，即如果对某一个特征的具体表现形式做处理。主要包括缺失值处理，特殊的特征处理比如时间和地理位置处理，离散特征的连续化和离散化处理，连续特征的离散化处理几个方面。 1. 缺失值处理 特征有缺失值    
特征工程之特征选择	2018年5月13日	57438	115	31	        特征工程是数据分析中最耗时间和精力的一部分工作，它不像算法和模型那样是确定的步骤，更多是工程上的经验和权衡。因此没有统一的方法。这里只是对一些常用的方法做一个总结。本文关注于特征选择部分。后面还有两篇会关注于特征表达和特征预处理。 1. 特征的来源 在做数据分析的时候，特征的来源一般有两块，一块是业    
用gensim学习word2vec	2017年8月3日	95612	104	24	        在word2vec原理篇中，我们对word2vec的两种模型CBOW和Skip-Gram，以及两种解法Hierarchical Softmax和Negative Sampling做了总结。这里我们就从实践的角度，使用gensim来学习word2vec。 1. gensim安装与概述 gensim是一    
word2vec原理(三) 基于Negative Sampling的模型	2017年7月28日	106072	131	27	        word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 在上一篇中我们讲到了基于Hierarchical Softmax的word2ve    
word2vec原理(二) 基于Hierarchical Softmax的模型	2017年7月27日	137971	290	45	        word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 在word2vec原理(一) CBOW与Skip-Gram模型基础中，我们讲到了    
word2vec原理(一) CBOW与Skip-Gram模型基础	2017年7月13日	254498	112	44	        word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 word2vec是google在2013年推出的一个NLP工具，它的特点是将所有    
条件随机场CRF(三) 模型学习与维特比算法解码	2017年6月23日	28683	66	7	        条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三)&#160;模型学习与维特比算法解码 在CRF系列的前两篇，我们总结了CRF的模型基础与第一个问题的求解方法，本文我们关注于linear-CRF的第二个问题与第三个问题的求解。第二个问    
条件随机场CRF(二) 前向后向算法评估标记序列概率	2017年6月22日	23250	80	5	        条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 在条件随机场CRF(一)中我们总结了CRF的模型，主要是linear-CRF的模型原理。本文就继续讨论linear-CRF需要解决的三个问题：评估    
条件随机场CRF(一)从随机场到线性链条件随机场	2017年6月19日	55032	99	21	        条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 条件随机场(Conditional Random Fields, 以下简称CRF)是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然    
用hmmlearn学习隐马尔科夫模型HMM	2017年6月13日	55092	190	15	        在之前的HMM系列中，我们对隐马尔科夫模型HMM的原理以及三个问题的求解方法做了总结。本文我们就从实践的角度用Python的hmmlearn库来学习HMM的使用。关于hmmlearn的更多资料在官方文档有介绍。 1.&#160;hmmlearn概述 hmmlearn安装很简单，&quot;pip install hmm    
隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列	2017年6月12日	43487	35	8	        隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在本篇我们会讨论HMM模型最后一个问题的求解，即即给定模型和观测序列，求给定观测序列条件下，最    
隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数	2017年6月10日	40579	97	12	        隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在本篇我们会讨论HMM模型参数求解的问题，这个问题在HMM三个问题里算是最复杂的。在研究这个问    
隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率	2017年6月8日	63331	61	20	        隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在隐马尔科夫模型HMM（一）HMM模型中，我们讲到了HMM模型的基础知识和HMM的三个基本问题    
隐马尔科夫模型HMM（一）HMM模型	2017年6月6日	120036	37	35	        隐马尔科夫模型HMM（一）HMM模型基础 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学    
EM算法原理总结	2017年5月27日	103761	131	33	        EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。本文就对EM算法的原理做一个总结。 1.&#160;EM算法要解决的问题 我们经常会从样本观察数据中，找出样本的模型参    
用scikit-learn学习LDA主题模型	2017年5月26日	67856	115	13	        在LDA模型原理篇我们总结了LDA主题模型的原理，这里我们就从应用的角度来使用scikit-learn来学习LDA主题模型。除了scikit-learn, 还有spark MLlib和gensim库也有LDA主题模型的类库，使用的原理基本类似，本文关注于scikit-learn中LDA主题模型的使用    
文本主题模型之LDA(三) LDA求解之变分推断EM算法	2017年5月22日	32855	101	11	        文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 本文是LDA主题模型的第三篇，读这一篇之前建议先读文本主题模型之LDA(一) LDA基础，同时由于使用了EM算法，如果你对EM算法不熟悉，建议    
文本主题模型之LDA(二) LDA求解之Gibbs采样算法	2017年5月18日	55410	218	10	        文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 本文是LDA主题模型的第二篇，读这一篇之前建议先读文本主题模型之LDA(一) LDA基础，同时由于使用了基于MCMC的Gibbs采样算法，如果    
文本主题模型之LDA(一) LDA基础	2017年5月17日	214245	105	29	        文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 在前面我们讲到了基于矩阵分解的LSI和NMF主题模型，这里我们开始讨论被广泛使用的主题模型：隐含狄利克雷分布(Latent Dirichlet    
文本主题模型之非负矩阵分解(NMF)	2017年5月5日	31403	15	9	        在文本主题模型之潜在语义索引(LSI)中，我们讲到LSI主题模型使用了奇异值分解，面临着高维度计算量太大的问题。这里我们就介绍另一种基于矩阵分解的主题模型：非负矩阵分解(NMF)，它同样使用了矩阵分解，但是计算量和处理速度则比LSI快，它是怎么做到的呢？ 1.&#160;非负矩阵分解(NMF)概述 非负矩阵分    
文本主题模型之潜在语义索引(LSI)	2017年5月4日	40598	48	11	        在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 1. 文本主题模型的问题特点 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型    
英文文本挖掘预处理流程总结	2017年4月24日	27722	16	9	        在中文文本挖掘预处理流程总结中，我们总结了中文文本挖掘的预处理流程，这里我们再对英文文本挖掘的预处理流程做一个总结。 1. 英文文本挖掘预处理特点 英文文本的预处理方法和中文的有部分区别。首先，英文文本挖掘预处理一般可以不做分词（特殊需求除外），而中文预处理分词是必不可少的一步。第二点，大部分英文文    
中文文本挖掘预处理流程总结	2017年4月21日	59172	76	20	        在对文本做数据分析时，我们一大半的时间都会花在文本预处理上，而中文和英文的预处理流程稍有不同，本文就对中文文本挖掘的预处理流程做一个总结。 1. 中文文本挖掘预处理特点 首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。 首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接    
文本挖掘预处理之TF-IDF	2017年4月11日	77757	39	20	        在文本挖掘预处理之向量化与Hash Trick中我们讲到在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。 1. 文本向量化特征的不足 在将文本分词并向量化后，我们可以得到词汇表中每个词在各    
文本挖掘预处理之向量化与Hash Trick	2017年4月10日	28280	36	8	        在文本挖掘的分词原理中，我们讲到了文本挖掘的预处理的关键一步：“分词”，而在做了分词后，如果我们是做文本分类聚类，则后面关键的特征预处理步骤有向量化或向量化的特例Hash Trick，本文我们就对向量化和特例Hash Trick预处理方法做一个总结。 1. 词袋模型 在讲向量化与Hash Trick    
文本挖掘的分词原理	2017年4月7日	37997	65	17	        在做文本挖掘的时候，首先要做的预处理就是分词。英文单词天然有空格隔开容易按照空格分词，但是也有时候需要把多个单词做为一个分词，比如一些名词如“New York”，需要做为一个词看待。而中文由于没有空格，分词就是一个需要专门去解决的问题了。无论是英文还是中文，分词的原理都是类似的，本文就对文本挖掘时的    
MCMC(四)Gibbs采样	2017年3月30日	102216	145	30	        MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(三)MCMC采样和M-H采样中，我们讲到了M-H采样已经可以很好的解决蒙特卡罗方法需要的任意概率分布的样本集的问题。但是M-H采样有两个缺点：一是需要计算接受率，在    
MCMC(三)MCMC采样和M-H采样	2017年3月29日	122735	267	34	        MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(二)马尔科夫链中我们讲到给定一个概率平稳分布$\pi$, 很难直接找到对应的马尔科夫链状态转移矩阵$P$。而只要解决这个问题，我们就可以找到一种通用的概率分布采样方    
MCMC(二)马尔科夫链	2017年3月28日	120969	150	49	        MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(一)蒙特卡罗方法中，我们讲到了如何用蒙特卡罗方法来随机模拟求解一些复杂的连续积分或者离散求和的方法，但是这个方法需要得到对应的概率分布的样本集，而想得到这样的样本集    
MCMC(一)蒙特卡罗方法	2017年3月27日	182371	116	69	        MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 作为一种随机采样方法，马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，以下简称MCMC）在机器学习,深度学习以及自然语言处理等领域都有广泛的应用，是很多复    
受限玻尔兹曼机（RBM）原理总结	2017年3月11日	48668	44	17	        在前面我们讲到了深度学习的两类神经网络模型的原理，第一类是前向的神经网络，即DNN和CNN。第二类是有反馈的神经网络，即RNN和LSTM。今天我们就总结下深度学习里的第三类神经网络模型：玻尔兹曼机。主要关注于这类模型中的受限玻尔兹曼机（Restricted Boltzmann Machine，以下简    
LSTM模型与前向反向传播算法	2017年3月8日	95387	173	34	        在循环神经网络(RNN)模型与前向反向传播算法中，我们总结了对RNN模型做了总结。由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。    
循环神经网络(RNN)模型与前向反向传播算法	2017年3月6日	156406	217	28	        在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。今天我们就讨论另一类输出和模型间有反馈的神经网络：循环神经网络(Recurrent Neural Networks ，以下简称RNN)，它广泛的用于自然语言处理中的语音识    
卷积神经网络(CNN)反向传播算法	2017年3月3日	204700	256	61	        在卷积神经网络(CNN)前向传播算法中，我们对CNN的前向传播算法做了总结，基于CNN前向传播算法的基础，我们下面就对CNN的反向传播算法做一个总结。在阅读本文前，建议先研究DNN的反向传播算法：深度神经网络（DNN）反向传播算法(BP) 1. 回顾DNN的反向传播算法 我们首先回顾DNN的反向传播    
卷积神经网络(CNN)前向传播算法	2017年3月2日	70604	56	17	        在卷积神经网络(CNN)模型结构中，我们对CNN的模型结构做了总结，这里我们就在CNN的模型基础上，看看CNN的前向传播算法是什么样子的。重点会和传统的DNN比较讨论。 1. 回顾CNN的结构 在上一篇里，我们已经讲到了CNN的结构，包括输出层，若干的卷积层+ReLU激活函数，若干的池化层，DNN全    
卷积神经网络(CNN)模型结构	2017年3月1日	212000	79	32	        在前面我们讲述了DNN的模型与前向反向传播算法。而在DNN大类中，卷积神经网络(Convolutional Neural Networks，以下简称CNN)是最为成功的DNN特例之一。CNN广泛的应用于图像识别，当然现在也应用于NLP等其他领域，本文我们就对CNN的模型结构做一个总结。 在学习CNN    
深度神经网络（DNN）的正则化	2017年2月27日	41052	40	13	        和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，这里我们就对DNN的正则化方法做一个总结。 1. DNN的L1&amp;L2正则化 想到正则化，我们首先想到的就是L1正则化和L2正则化。L1正则化和L2正则化原理类似，这里重点讲述DNN的L2正则化。 而DNN的L2正则化通常的做法是只针    
深度神经网络（DNN）损失函数和激活函数的选择	2017年2月24日	87295	164	15	        在深度神经网络（DNN）反向传播算法(BP)中，我们对DNN的前向反向传播算法的使用做了总结。里面使用的损失函数是均方差，而激活函数是Sigmoid。实际上DNN可以使用的损失函数和激活函数不少。这些损失函数和激活函数如何选择呢？下面我们就对DNN损失函数和激活函数的选择做一个总结。 1. 均方差损    
深度神经网络（DNN）反向传播算法(BP)	2017年2月21日	133178	176	40	        在深度神经网络（DNN）模型与前向传播算法中，我们对DNN的模型和前向传播算法做了总结，这里我们更进一步，对DNN的反向传播算法（Back Propagation，BP）做一个总结。 1. DNN反向传播算法要解决的问题 在了解DNN的反向传播算法前，我们先要知道DNN反向传播算法要解决的问题，也就    
深度神经网络（DNN）模型与前向传播算法	2017年2月20日	222842	47	52	        深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。 1. 从感知机到神经网络 在感知机原理小结中，我们介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:    
分解机(Factorization Machines)推荐算法原理	2017年2月6日	52094	72	11	        对于分解机(Factorization Machines，FM)推荐算法原理，本来想自己单独写一篇的。但是看到peghoty写的FM不光简单易懂，而且排版也非常好，因此转载过来，自己就不再单独写FM了。 Pinard注：上面最后一句话应该是&quot;而$g_{\theta}(x)$则利用$\widehat{    
用Spark学习矩阵分解推荐算法	2017年2月4日	21933	44	6	        在矩阵分解在协同过滤推荐算法中的应用中，我们对矩阵分解在推荐算法中的应用原理做了总结，这里我们就从实践的角度来用Spark学习矩阵分解推荐算法。 1. Spark推荐算法概述 在Spark MLlib中，推荐算法这块只实现了基于矩阵分解的协同过滤推荐算法。而基于的算法是FunkSVD算法，即将m个用    
SimRank协同过滤推荐算法	2017年2月3日	20002	36	5	        在协同过滤推荐算法总结中，我们讲到了用图模型做协同过滤的方法，包括SimRank系列算法和马尔科夫链系列算法。现在我们就对SimRank算法在推荐系统的应用做一个总结。 1.&#160;SimRank推荐算法的图论基础 SimRank是基于图论的，如果用于推荐算法，则它假设用户和物品在空间中形成了一张图。而这    
矩阵分解在协同过滤推荐算法中的应用	2017年1月26日	48157	86	19	        在协同过滤推荐算法总结中，我们讲到了用矩阵分解做协同过滤是广泛使用的方法，这里就对矩阵分解在协同过滤推荐算法中的应用做一个总结。(过年前最后一篇！祝大家新年快乐！明年的目标是写120篇机器学习，深度学习和NLP相关的文章) 1.&#160;矩阵分解用于推荐算法要解决的问题 在推荐系统中，我们常常遇到的问题是这    
协同过滤推荐算法总结	2017年1月25日	95539	66	41	        推荐算法具有非常多的应用场景和商业价值，因此对推荐算法值得好好研究。推荐算法种类很多，但是目前应用最广泛的应该是协同过滤类别的推荐算法，本文就对协同过滤类别的推荐算法做一个概括总结，后续也会对一些典型的协同过滤推荐算法做原理总结。 1. 推荐算法概述 推荐算法是非常古老的，在机器学习还没有兴起的时候    
用Spark学习FP Tree算法和PrefixSpan算法	2017年1月22日	14291	24	4	        在FP Tree算法原理总结和PrefixSpan算法原理总结中，我们对FP Tree和PrefixSpan这两种关联算法的原理做了总结，这里就从实践的角度介绍如何使用这两个算法。由于scikit-learn中没有关联算法的类库，而Spark MLlib有，本文的使用以Spark MLlib作为使用    
PrefixSpan算法原理总结	2017年1月20日	39794	32	8	        前面我们讲到频繁项集挖掘的关联算法Apriori和FP Tree。这两个算法都是挖掘频繁项集的。而今天我们要介绍的PrefixSpan算法也是关联算法，但是它是挖掘频繁序列模式的，因此要解决的问题目标稍有不同。 1.&#160;项集数据和序列数据 首先我们看看项集数据和序列数据有什么不同，如下图所示。 左边的    
FP Tree算法原理总结	2017年1月19日	84920	80	48	        在Apriori算法原理总结中，我们对Apriori算法的原理做了总结。作为一个挖掘频繁项集的算法，Apriori算法需要多次扫描数据，I/O是很大的瓶颈。为了解决这个问题，FP Tree算法（也称FP Growth算法）采用了一些技巧，无论多少数据，只需要扫描两次数据集，因此提高了算法运行的效率。    
Apriori算法原理总结	2017年1月17日	123925	52	31	        Apriori算法是常用的用于挖掘出数据关联规则的算法，它用来找出数据值中频繁出现的数据集合，找出这些集合的模式有助于我们做一些决策。比如在常见的超市购物数据集，或者电商的网购数据集中，如果我们找到了频繁出现的数据集，那么对于超市，我们可以优化产品的位置摆放，对于电商，我们可以优化商品所在的仓库位置    
典型关联分析(CCA)原理总结	2017年1月16日	70378	59	20	        典型关联分析(Canonical Correlation Analysis，以下简称CCA)是最常用的挖掘数据关联关系的算法之一。比如我们拿到两组数据，第一组是人身高和体重的数据，第二组是对应的跑步能力和跳远能力的数据。那么我们能不能说这两组数据是相关的呢？CCA可以帮助我们分析这个问题。 1. C    
用scikit-learn研究局部线性嵌入(LLE)	2017年1月11日	11599	0	5	        在局部线性嵌入(LLE)原理总结中，我们对流形学习中的局部线性嵌入(LLE)算法做了原理总结。这里我们就对scikit-learn中流形学习的一些算法做一个介绍，并着重对其中LLE算法的使用方法做一个实践上的总结。 1.&#160;scikit-learn流形学习库概述 在scikit-learn中，流形学习    
局部线性嵌入(LLE)原理总结	2017年1月10日	68327	94	15	        局部线性嵌入(Locally Linear Embedding，以下简称LLE)也是非常重要的降维方法。和传统的PCA，LDA等关注样本方差的降维方法相比，LLE关注于降维时保持样本局部的线性特征，由于LLE在降维时保持了样本的局部特征，它广泛的用于图像图像识别，高维数据可视化等领域。下面我们就对L    
奇异值分解(SVD)原理与在降维中的应用	2017年1月5日	291471	125	124	        奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SV    
用scikit-learn进行LDA降维	2017年1月4日	42552	29	8	        在线性判别分析LDA原理总结中，我们对LDA降维的原理做了总结，这里我们就对scikit-learn中LDA的降维使用做一个总结。 1.&#160;对scikit-learn中LDA类概述 在scikit-learn中， LDA类是sklearn.discriminant_analysis.LinearDis    
线性判别分析LDA原理总结	2017年1月3日	301137	207	57	        在主成分分析（PCA）原理总结中，我们对降维算法PCA做了总结。这里我们就对另外一种经典的降维方法线性判别分析（Linear Discriminant Analysis, 以下简称LDA）做一个总结。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了    
用scikit-learn学习主成分分析(PCA)	2017年1月2日	157638	74	18	        在主成分分析（PCA）原理总结中，我们对主成分分析(以下简称PCA)的原理做了总结，下面我们就总结下如何使用scikit-learn工具来进行PCA降维。 1. scikit-learn PCA类介绍 在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。    
主成分分析（PCA）原理总结	2016年12月31日	238963	257	68	        主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。一般我们提到降维最容易想到的算法就是PCA，下面我们就对PCA的原理做一个总结。 1. PCA的思想 PCA顾名思义，就是找出数据里最    
用scikit-learn学习谱聚类	2016年12月30日	42765	47	8	        在谱聚类（spectral clustering）原理总结中，我们对谱聚类的原理做了总结。这里我们就对scikit-learn中谱聚类的使用做一个总结。 1. scikit-learn谱聚类概述 在scikit-learn的类库中，sklearn.cluster.SpectralClustering    
谱聚类（spectral clustering）原理总结	2016年12月29日	334131	299	88	        谱聚类（spectral clustering）是广泛使用的聚类算法，比起传统的K-Means算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多，更加难能可贵的是实现起来也不复杂。在处理实际的聚类问题时，个人认为谱聚类是应该首先考虑的几种算法之一。下面我们就对谱聚类的算法    
用scikit-learn学习DBSCAN聚类	2016年12月24日	129251	78	14	        在DBSCAN密度聚类算法中，我们对DBSCAN聚类算法的原理做了总结，本文就对如何用scikit-learn来学习DBSCAN聚类做一个总结，重点讲述参数的意义和需要调参的参数。 1. scikit-learn中的DBSCAN类 在scikit-learn中，DBSCAN算法类为sklearn.c    
DBSCAN密度聚类算法	2016年12月22日	258316	80	38	        DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用    
用scikit-learn学习BIRCH聚类	2016年12月19日	26142	55	5	        在BIRCH聚类算法原理中，我们对BIRCH聚类算法的原理做了总结，本文就对scikit-learn中BIRCH算法的使用做一个总结。 1. scikit-learn之BIRCH类 在scikit-learn中，BIRCH类实现了原理篇里讲到的基于特征树CF Tree的聚类。因此要使用BIRCH来聚    
BIRCH聚类算法原理	2016年12月14日	76741	74	25	        在K-Means聚类算法原理中，我们讲到了K-Means和Mini Batch K-Means的聚类原理。这里我们再来看看另外一种常见的聚类算法BIRCH。BIRCH算法比较适合于数据量大，类别数K也比较多的情况。它运行速度很快，只需要单遍扫描数据集就能进行聚类，当然需要用到一些技巧，下面我们就对B    
用scikit-learn学习K-Means聚类	2016年12月13日	119953	73	17	        在K-Means聚类算法原理中，我们对K-Means的原理做了总结，本文我们就来讨论用scikit-learn来学习K-Means聚类。重点讲述如何选择合适的k值。 1. K-Means类概述 在scikit-learn中，包括两个K-Means的算法，一个是传统的K-Means算法，对应的类是KM    
K-Means聚类算法原理	2016年12月12日	334815	79	41	        K-Means算法是无监督的聚类算法，它实现起来比较简单，聚类效果也不错，因此应用很广泛。K-Means算法有大量的变体，本文就从最传统的K-Means算法讲起，在其基础上讲述K-Means的优化变体方法。包括初始化优化K-Means++, 距离计算优化elkan K-Means算法和大数据情况下的    
scikit-learn随机森林调参小结	2016年12月11日	165920	166	34	        在Bagging与随机森林算法原理小结中，我们对随机森林(Random Forest, 以下简称RF）的原理做了总结。本文就从实践的角度对RF做一个总结。重点讲述scikit-learn中RF的调参注意事项，以及和GBDT调参的异同点。 1. scikit-learn随机森林类库概述 在scikit    
Bagging与随机森林算法原理小结	2016年12月10日	138372	165	35	        在集成学习原理小结中，我们讲到了集成学习有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。本文就对集成学习中Bagging与随机森林算法做一个总结。 随机森林是集成学习中可以和梯度提升树GB    
scikit-learn 梯度提升树(GBDT)调参小结	2016年12月9日	132922	108	26	        在梯度提升树(GBDT)原理小结中，我们对GBDT的原理做了总结，本文我们就从scikit-learn里GBDT的类库使用方法作一个总结，主要会关注调参中的一些要点。 1.&#160;scikit-learn GBDT类库概述 在sacikit-learn中，GradientBoostingClassifie    
梯度提升树(GBDT)原理小结	2016年12月7日	340331	617	67	        在集成学习之Adaboost算法原理小结中，我们对Boosting家族的Adaboost算法做了总结，本文就对Boosting家族中另一个重要的算法梯度提升树(Gradient Boosting Decison Tree, 以下简称GBDT)做一个总结。GBDT有很多简称，有GBT（Gradient    
scikit-learn Adaboost类库使用小结	2016年12月6日	100244	118	31	        在集成学习之Adaboost算法原理小结中，我们对Adaboost的算法原理做了一个总结。这里我们就从实用的角度对scikit-learn中Adaboost类库的使用做一个小结，重点对调参的注意事项做一个总结。 1. Adaboost类库概述 scikit-learn中Adaboost类库比较直接，    
集成学习之Adaboost算法原理小结	2016年12月5日	169614	362	70	        在集成学习原理小结中，我们讲到了集成学习按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，另一类是个体学习器之间不存在强依赖关系。前者的代表算法就是是boosting系列算法。在boosting系列算法中， Adaboost是最著名的算法之一。Adaboost既可    
集成学习原理小结	2016年12月4日	103103	55	67	        集成学习(ensemble learning)可以说是现在非常火爆的机器学习方法了。它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。也就是我们常说的“博采众长”。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等等，可以说所有的机器学习领域都    
支持向量机高斯核调参小结	2016年12月2日	42475	60	18	        在支持向量机(以下简称SVM)的核函数中，高斯核(以下简称RBF)是最常用的，从理论上讲， RBF一定不比线性核函数差，但是在实际应用中，却面临着几个重要的超参数的调优问题。如果调的不好，可能比线性核函数还要差。所以我们实际应用中，能用线性核函数得到较好效果的都会选择线性核函数。如果线性核不好，我们    
scikit-learn 支持向量机算法库使用小结	2016年11月30日	47873	40	12	        之前通过一个系列对支持向量机(以下简称SVM)算法的原理做了一个总结，本文从实践的角度对scikit-learn SVM算法库的使用做一个小结。scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分。 1.&#160;scikit-learn SVM    
支持向量机原理(五)线性支持回归	2016年11月29日	43065	41	11	        支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在前四篇里面我们讲到了SVM的线性分类和非线性分类，以及在分类时用到的算法。这些都关注    
支持向量机原理(三)线性不可分支持向量机与核函数	2016年11月26日	37387	43	14	        支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在前面两篇我们讲到了线性可分SVM的硬间隔最大化和软间隔最大化的算法，它们对线性可分的    
支持向量机原理(二) 线性支持向量机的软间隔最大化模型	2016年11月25日	44738	73	20	        支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在支持向量机原理(一) 线性支持向量机中，我们对线性可分SVM的模型和损失函数优化做了    
支持向量机原理(一) 线性支持向量机	2016年11月24日	118462	124	47	        支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 支持向量机(Support Vecor Machine,以下简称SVM)虽然诞生只有短    
最大熵模型原理小结	2016年11月23日	52743	100	15	        最大熵模型(maximum entropy model， MaxEnt)也是很典型的分类算法了，它和逻辑回归类似，都是属于对数线性分类模型。在损失函数优化的过程中，使用了和支持向量机类似的凸优化技术。而对熵的使用，让我们想起了决策树算法中的ID3和C4.5算法。理解了最大熵模型，对逻辑回归，支持向量    
scikit-learn 朴素贝叶斯类库使用小结	2016年11月17日	57597	42	22	        之前在朴素贝叶斯算法原理小结这篇文章中，对朴素贝叶斯分类算法的原理做了一个总结。这里我们就从实战的角度来看朴素贝叶斯类库。重点讲述scikit-learn 朴素贝叶斯类库的使用要点和参数选择。 1. scikit-learn 朴素贝叶斯类库概述 朴素贝叶斯是一类比较简单的算法，scikit-lear    
朴素贝叶斯算法原理小结	2016年11月16日	131611	130	48	        在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数$Y=f(X)$,要么是条件分布$P(Y|X)$。但是朴素贝叶斯却是生成方法，也就是直    
scikit-learn K近邻法类库使用小结	2016年11月15日	39086	13	10	        在K近邻法(KNN)原理小结这篇文章，我们讨论了KNN的原理和优缺点，这里我们就从实践出发，对scikit-learn 中KNN相关的类库使用做一个小结。主要关注于类库调参时的一个经验总结。 1.&#160;scikit-learn 中KNN相关的类库概述 在scikit-learn 中，与近邻法这一大类相关    
K近邻法(KNN)原理小结	2016年11月14日	87949	87	28	        K近邻法(k-nearest neighbors,KNN)是一种很基本的机器学习方法了，在我们平常的生活中也会不自主的应用。比如，我们判断一个人的人品，只需要观察他来往最密切的几个人的人品好坏就可以得出了。这里就运用了KNN的思想。KNN方法既可以做分类，也可以做回归，这点和决策树算法相同。 KNN    
scikit-learn决策树算法类库使用小结	2016年11月12日	149409	137	30	        之前对决策树的算法原理做了总结，包括决策树算法原理(上)和决策树算法原理(下)。今天就从实践的角度来介绍决策树算法，主要是讲解使用scikit-learn来跑决策树算法，结果的可视化以及一些参数调参的关键点。 1.&#160;scikit-learn决策树算法类库介绍 scikit-learn决策树算法类库内    
决策树算法原理(下)	2016年11月11日	130636	342	52	        在决策树算法原理(上)这篇里，我们讲到了决策树里ID3算法，和ID3算法的改进版C4.5算法。对于C4.5算法，我们也提到了它的不足，比如模型是用较为复杂的熵来度量，使用了相对较为复杂的多叉树，只能处理分类不能处理回归等。对于这些问题， CART算法大部分做了改进。CART算法也就是我们下面的重点了    
决策树算法原理(上)	2016年11月10日	136562	144	50	        决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。本文就对决策树算法原理做一个总结，上篇对ID3， C4.5的算法思想做了总结，下篇重点对CART算法做一个详细的介绍。选择CART做重点介绍的原因是scikit-learn    
机器学习算法的随机数据生成	2016年11月9日	35570	11	20	        在学习机器学习算法的过程中，我们经常需要数据来验证算法，调试参数。但是找到一组十分合适某种特定算法类型的数据样本却不那么容易。还好numpy, scikit-learn都提供了随机数据生成的功能，我们可以自己生成适合某一种模型的数据，用随机数据来做清洗，归一化，转换，然后选择模型与算法做拟合和预测。    
感知机原理小结	2016年11月8日	59596	115	41	        感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。这里对感知机的原理做一个小结。     
日志和告警数据挖掘经验谈	2016年11月7日	18545	17	5	        最近参与了了一个日志和告警的数据挖掘项目，里面用到的一些思路在这里和大家做一个分享。 项目的需求是收集的客户系统一个月300G左右的的日志和告警数据做一个整理，主要是归类(Grouping)和关联(Correlation)，从而得到告警和日志的一些统计关系，这些统计结果可以给一线支持人员参考。 得到    
scikit-learn 逻辑回归类库使用小结	2016年11月6日	51766	66	17	        之前在逻辑回归原理小结这篇文章中，对逻辑回归的原理做了小结。这里接着对scikit-learn中逻辑回归类库的我的使用经验做一个总结。重点讲述调参中要注意的事项。 1. 概述 在scikit-learn中，与逻辑回归有关的主要是这3个类。LogisticRegression， LogisticReg    
逻辑回归原理小结	2016年11月4日	109615	199	41	        逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。虽然它名字里面有“回归”两个字，却不是一个回归算法。那为什么有“回归”这个误导性的词呢？个人认为，虽然逻辑回归是分类模型，但是它的原理里面却残留着回归模型的影子，本文对逻辑回归原理做一个总结。 1. 从线性回归到逻辑回归 我们知道，线性回归的模    
scikit-learn 线性回归算法库小结	2016年11月3日	36470	27	13	        scikit-learn对于线性回归提供了比较多的类库，这些类库都可以用来做线性回归分析，本文就对这些类库的使用做一个总结，重点讲述这些线性回归算法库的不同和各自的使用场景。 线性回归的目的是要得到输出向量\(\mathbf{Y}\)和输入特征\(\mathbf{X}\)之间的线性关系，求出线性回归    
用scikit-learn和pandas学习Ridge回归	2016年11月2日	30800	44	11	        本文将用一个例子来讲述怎么用scikit-learn和pandas来学习Ridge回归。 1. Ridge回归的损失函数 在我的另外一遍讲线性回归的文章中，对Ridge回归做了一些介绍，以及什么时候适合用 Ridge回归。如果对什么是Ridge回归还完全不清楚的建议阅读我这篇文章。 线性回归原理小结    
Lasso回归算法： 坐标轴下降法与最小角回归法小结	2016年11月1日	71729	73	24	        前面的文章对线性回归做了一个小结，文章在这：&#160;线性回归原理小结。里面对线程回归的正则化也做了一个初步的介绍。提到了线程回归的L2正则化-Ridge回归，以及线程回归的L1正则化-Lasso回归。但是对于Lasso回归的解法没有提及，本文是对该文的补充和扩展。以下都用矩阵法表示，如果对于矩阵分析不熟悉    
用scikit-learn和pandas学习线性回归	2016年10月31日	105266	77	21	        对于想深入了解线性回归的童鞋，这里给出一个完整的例子，详细学完这个例子，对用scikit-learn来运行线性回归，评估模型不会有什么问题了。 1. 获取数据，定义问题 没有数据，当然没法研究机器学习啦。:) 这里我们用UCI大学公开的机器学习数据来跑线性回归。 数据的介绍在这：&#160;http://ar    
scikit-learn 和pandas 基于windows单机机器学习环境的搭建	2016年10月30日	17691	8	4	        很多朋友想学习机器学习，却苦于环境的搭建，这里给出windows上scikit-learn研究开发环境的搭建步骤。 Step 1. Python的安装 python有2.x和3.x的版本之分，但是很多好的机器学习python库都不支持3.x，因此，推荐安装2.7版本的python。当前最新的pyth    
机器学习研究与开发平台的选择	2016年10月28日	24103	30	15	        目前机器学习可以说是百花齐放阶段，不过如果要学习或者研究机器学习，进而用到生产环境，对平台，开发语言，机器学习库的选择就要费一番脑筋了。这里就我自己的机器学习经验做一个建议，仅供参考。 首先，对于平台选择的第一个问题是，你是要用于生产环境，也就是具体的产品中,还是仅仅是做研究学习用？ 1. 生产环境    
线性回归原理小结	2016年10月28日	60826	111	24	        线性回归可以说是机器学习中最基本的问题类型了，这里就对线性回归的原理和算法做一个小结。 1. 线性回归的模型函数和损失函数 线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应于n维特征和一个结果输出，如下： \((x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_    
精确率与召回率，RoC曲线与PR曲线	2016年10月24日	42227	35	16	        在机器学习的算法评估中，尤其是分类算法评估中，我们经常听到精确率(precision)与召回率(recall)，RoC曲线与PR曲线这些概念，那这些概念到底有什么用处呢？ 首先，我们需要搞清楚几个拗口的概念： 1. TP, FP, TN, FN 听起来还是很费劲，不过我们用一张图就很容易理解了。图如    
最小二乘法小结	2016年10月19日	108660	72	58	        最小二乘法是用来做函数拟合或者求函数极值的方法。在机器学习，尤其是回归模型中，经常可以看到最小二乘法的身影，这里就对我对最小二乘法的认知做一个小结。 1.最小二乘法的原理与要解决的问题 最小二乘法是由勒让德在19世纪发现的，原理的一般形式很简单，当然发现的过程是非常艰难的。形式如下式：$$目标函数     
梯度下降（Gradient Descent）小结	2016年10月17日	544352	247	166	        在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。 1. 梯度 在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数    
XGBoost类库使用小结	2019年7月1日	66979	135	21	        在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth    
XGBoost算法原理小结	2019年6月5日	78759	201	36	        在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P    
机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导	2019年5月27日	43338	27	7	        在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章    
机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则	2019年5月7日	57463	71	29	        在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果    
机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法	2019年4月29日	41674	84	19	        在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量    
机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法	2019年4月26日	43322	46	22	        在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导    
机器学习中的矩阵向量求导(一) 求导定义与求导布局	2019年4月22日	67941	19	55	        在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的    
强化学习(十九) AlphaGo Zero强化学习原理	2019年3月27日	40211	69	14	        在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG    
强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)	2019年3月4日	50054	29	5	        在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参    
强化学习(十七) 基于模型的强化学习与Dyna算法框架	2019年2月15日	25649	26	2	        在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy    
XGBoost类库使用小结	2019年7月1日	66979	135	21	        在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth    
XGBoost算法原理小结	2019年6月5日	78759	201	36	        在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P    
机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导	2019年5月27日	43338	27	7	        在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章    
机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则	2019年5月7日	57463	71	29	        在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果    
机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法	2019年4月29日	41674	84	19	        在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量    
机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法	2019年4月26日	43322	46	22	        在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导    
机器学习中的矩阵向量求导(一) 求导定义与求导布局	2019年4月22日	67941	19	55	        在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的    
强化学习(十九) AlphaGo Zero强化学习原理	2019年3月27日	40211	69	14	        在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG    
强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)	2019年3月4日	50054	29	5	        在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参    
强化学习(十七) 基于模型的强化学习与Dyna算法框架	2019年2月15日	25649	26	2	        在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy    
XGBoost类库使用小结	2019年2月15日	66979	135	21	        在XGBoost算法原理小结中，我们讨论了XGBoost的算法原理，这一片我们讨论如何使用XGBoost的Python类库，以及一些重要参数的意义和调参思路。 本文主要参考了XGBoost的Python文档&#160;和&#160;XGBoost的参数文档。 1. XGBoost类库概述 XGBoost除了支持Pyth    
XGBoost算法原理小结	2019年2月15日	78759	201	36	        在两年半之前作过梯度提升树(GBDT)原理小结，但是对GBDT的算法库XGBoost没有单独拿出来分析。虽然XGBoost是GBDT的一种高效实现，但是里面也加入了很多独有的思路和方法，值得单独讲一讲。因此讨论的时候，我会重点分析和GBDT不同的地方。 本文主要参考了XGBoost的论文和陈天奇的P    
机器学习中的矩阵向量求导(五) 矩阵对矩阵的求导	2019年2月15日	43338	27	7	        在矩阵向量求导前4篇文章中，我们主要讨论了标量对向量矩阵的求导，以及向量对向量的求导。本文我们就讨论下之前没有涉及到的矩阵对矩阵的求导，还有矩阵对向量，向量对矩阵求导这几种形式的求导方法。 本文所有求导布局以分母布局为准，为了适配矩阵对矩阵的求导，本文向量对向量的求导也以分母布局为准，这和前面的文章    
机器学习中的矩阵向量求导(四) 矩阵向量求导链式法则	2019年2月15日	57463	71	29	        在机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法中，我们讨论了使用微分法来求解矩阵向量求导的方法。但是很多时候，求导的自变量和因变量直接有复杂的多层链式求导的关系，此时微分法使用起来也有些麻烦。需要一些简洁的方法。 本文我们讨论矩阵向量求导链式法则，使用该法则很多时候可以帮我们快速求出导数结果    
机器学习中的矩阵向量求导(三) 矩阵向量求导之微分法	2019年2月15日	41674	84	19	        在机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法中，我们讨论了定义法求解矩阵向量求导的方法，但是这个方法对于比较复杂的求导式子，中间运算会很复杂，同时排列求导出的结果也很麻烦。因此我们需要其他的一些求导方法。本文我们讨论使用微分法来求解标量对向量的求导，以及标量对矩阵的求导。 本文的标量对向量    
机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法	2019年2月15日	43322	46	22	        在机器学习中的矩阵向量求导(一) 求导定义与求导布局中，我们讨论了向量矩阵求导的9种定义与求导布局的概念。今天我们就讨论下其中的标量对向量求导，标量对矩阵求导, 以及向量对向量求导这三种场景的基本求解思路。 对于本文中的标量对向量或矩阵求导这两种情况，如前文所说，以分母布局为默认布局。向量对向量求导    
机器学习中的矩阵向量求导(一) 求导定义与求导布局	2019年2月15日	67941	19	55	        在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。 本系列主要参考文献为维基百科的Matrix Caculas和张贤达的    
强化学习(十九) AlphaGo Zero强化学习原理	2019年2月15日	40211	69	14	        在强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)中，我们讨论了MCTS的原理和在棋类中的基本应用。这里我们在前一节MCTS的基础上，讨论下DeepMind的AlphaGo Zero强化学习原理。 本篇主要参考了AlphaGo Zero的论文, AlphaGo Zero综述和AlphaG    
强化学习(十八) 基于模拟的搜索与蒙特卡罗树搜索(MCTS)	2019年2月15日	50054	29	5	        在强化学习(十七) 基于模型的强化学习与Dyna算法框架中，我们讨论基于模型的强化学习方法的基本思路，以及集合基于模型与不基于模型的强化学习框架Dyna。本文我们讨论另一种非常流行的集合基于模型与不基于模型的强化学习方法：基于模拟的搜索(Simulation Based Search)。 本篇主要参    
强化学习(十七) 基于模型的强化学习与Dyna算法框架	2019年2月15日	25649	26	2	        在前面我们讨论了基于价值的强化学习(Value Based RL)和基于策略的强化学习模型(Policy Based RL)，本篇我们讨论最后一种强化学习流派，基于模型的强化学习(Model Based RL)，以及基于模型的强化学习算法框架Dyna。 本篇主要参考了UCL强化学习课程的第8讲和Dy    
强化学习(十六) 深度确定性策略梯度(DDPG)	2019年2月15日	124117	318	25	        在强化学习(十五) A3C中，我们讨论了使用多线程的方法来解决Actor-Critic难收敛的问题，今天我们不使用多线程，而是使用和DDQN类似的方法：即经验回放和双网络的方法来改进Actor-Critic难收敛的问题，这个算法就是是深度确定性策略梯度(Deep Deterministic Poli    
强化学习(十五) A3C	2019年2月15日	71796	144	5	        在强化学习(十四) Actor-Critic中，我们讨论了Actor-Critic的算法流程，但是由于普通的Actor-Critic算法难以收敛，需要一些其他的优化。而Asynchronous Advantage Actor-critic(以下简称A3C)就是其中比较好的优化算法。本文我们讨论A3C    
强化学习(十四) Actor-Critic	2019年2月15日	115531	148	10	        在强化学习(十三) 策略梯度(Policy Gradient)中，我们讲到了基于策略(Policy Based)的强化学习方法的基本思路，并讨论了蒙特卡罗策略梯度reinforce算法。但是由于该算法需要完整的状态序列，同时单独对策略函数进行迭代更新，不太容易收敛。 在本篇我们讨论策略(Policy    
强化学习(十三) 策略梯度(Policy Gradient)	2019年2月15日	123030	177	14	        在前面讲到的DQN系列强化学习算法中，我们主要对价值函数进行了近似表示，基于价值来学习。这种Value Based强化学习方法在很多领域都得到比较好的应用，但是Value Based强化学习方法也有很多局限性，因此在另一些场景下我们需要其他的方法，比如本篇讨论的策略梯度(Policy Gradien    
强化学习(十二) Dueling DQN	2019年2月15日	58665	74	5	        在强化学习(十一) Prioritized Replay DQN中，我们讨论了对DQN的经验回放池按权重采样来优化DQN算法的方法，本文讨论另一种优化方法，Dueling DQN。本章内容主要参考了ICML 2016的deep RL tutorial和Dueling DQN的论文&lt;Dueling N    
强化学习(十一) Prioritized Replay DQN	2019年2月15日	52924	153	14	        在强化学习（十）Double DQN (DDQN)中，我们讲到了DDQN使用两个Q网络，用当前Q网络计算最大Q值对应的动作，用目标Q网络计算这个最大动作对应的目标Q值，进而消除贪婪法带来的偏差。今天我们在DDQN的基础上，对经验回放部分的逻辑做优化。对应的算法是Prioritized Replay     
强化学习（十）Double DQN (DDQN)	2019年2月15日	109163	77	8	        在强化学习（九）Deep Q-Learning进阶之Nature DQN中，我们讨论了Nature DQN的算法流程，它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性。但是还是有其他值得优化的点，文本就关注于Nature DQN的一个改进版本: Double DQN算法（以下简称D    
强化学习（九）Deep Q-Learning进阶之Nature DQN	2019年2月15日	69650	84	12	        在强化学习（八）价值函数的近似表示与Deep Q-Learning中，我们讲到了Deep Q-Learning（NIPS 2013）的算法和代码，在这个算法基础上，有很多Deep Q-Learning(以下简称DQN)的改进版，今天我们来讨论DQN的第一个改进版Nature DQN(NIPS 201    
强化学习（八）价值函数的近似表示与Deep Q-Learning	2019年2月15日	89834	203	14	        在强化学习系列的前七篇里，我们主要讨论的都是规模比较小的强化学习问题求解算法。今天开始我们步入深度强化学习。这一篇关注于价值函数的近似表示和Deep Q-Learning算法。 Deep&#160;Q-Learning这一篇对应Sutton书的第11章部分和UCL强化学习课程的第六讲。 1. 为何需要价值函数    
强化学习（七）时序差分离线控制算法Q-Learning	2019年2月15日	61739	109	15	        在强化学习（六）时序差分在线控制算法SARSA中我们讨论了时序差分的在线控制算法SARSA，而另一类时序差分的离线控制算法还没有讨论，因此本文我们关注于时序差分离线控制算法，主要是经典的Q-Learning算法。 Q-Learning这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部    
强化学习（六）时序差分在线控制算法SARSA	2019年2月15日	60953	87	10	        在强化学习（五）用时序差分法（TD）求解中，我们讨论了用时序差分来求解强化学习预测问题的方法，但是对控制算法的求解过程没有深入，本文我们就对时序差分的在线控制算法SARSA做详细的讨论。 SARSA这一篇对应Sutton书的第六章部分和UCL强化学习课程的第五讲部分。 1.&#160;SARSA算法的引入 S    
强化学习（五）用时序差分法（TD）求解	2019年2月15日	77983	131	16	        在强化学习（四）用蒙特卡罗法（MC）求解中，我们讲到了使用蒙特卡罗法来求解强化学习问题的方法，虽然蒙特卡罗法很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果我们没有完整的状态序列，那么就无法使用蒙特卡罗法求解了。本文我们就来讨论可以不使用完整状态序列求解强化    
强化学习（四）用蒙特卡罗法（MC）求解	2019年2月15日	73502	108	18	        在强化学习（三）用动态规划（DP）求解中，我们讨论了用动态规划来求解强化学习预测问题和控制问题的方法。但是由于动态规划法需要在每一次回溯更新某一个状态的价值时，回溯到该状态的所有可能的后续状态。导致对于复杂问题计算量很大。同时很多时候，我们连环境的状态转化模型$P$都无法知道，这时动态规划法根本没法    
强化学习（三）用动态规划（DP）求解	2019年2月15日	74705	103	22	        在强化学习（二）马尔科夫决策过程(MDP)中，我们讨论了用马尔科夫假设来简化强化学习模型的复杂度，这一篇我们在马尔科夫假设和贝尔曼方程的基础上讨论使用动态规划(Dynamic Programming, DP)来求解强化学习的问题。 动态规划这一篇对应Sutton书的第四章和UCL强化学习课程的第三讲    
强化学习（二）马尔科夫决策过程(MDP)	2019年2月15日	159663	142	26	        在强化学习（一）模型基础中，我们讲到了强化学习模型的8个基本要素。但是仅凭这些要素还是无法使用强化学习来帮助我们解决问题的, 在讲到模型训练前，模型的简化也很重要，这一篇主要就是讲如何利用马尔科夫决策过程(Markov Decision Process，以下简称MDP)来简化强化学习的建模。 MDP    
强化学习（一）模型基础	2019年2月15日	154851	75	36	        从今天开始整理强化学习领域的知识，主要参考的资料是Sutton的强化学习书和UCL强化学习的课程。这个系列大概准备写10到20篇，希望写完后自己的强化学习碎片化知识可以得到融会贯通，也希望可以帮到更多的人，毕竟目前系统的讲解强化学习的中文资料不太多。 第一篇会从强化学习的基本概念讲起，对应Sutto    
异常点检测算法小结	2019年2月15日	52287	78	11	        异常点检测，有时也叫离群点检测，英文一般叫做Novelty Detection或者Outlier Detection,是比较常见的一类非监督学习算法，这里就对异常点检测算法做一个总结。 1.&#160;异常点检测算法使用场景 什么时候我们需要异常点检测算法呢？常见的有三种情况。一是在做特征工程的时候需要对异常    
tensorflow机器学习模型的跨平台上线	2019年2月15日	14312	18	2	        在用PMML实现机器学习模型的跨平台上线中，我们讨论了使用PMML文件来实现跨平台模型上线的方法，这个方法当然也适用于tensorflow生成的模型，但是由于tensorflow模型往往较大，使用无法优化的PMML文件大多数时候很笨拙，因此本文我们专门讨论下tensorflow机器学习模型的跨平台上    
用PMML实现机器学习模型的跨平台上线	2019年2月15日	43263	79	15	        在机器学习用于产品的时候，我们经常会遇到跨平台的问题。比如我们用Python基于一系列的机器学习库训练了一个模型，但是有时候其他的产品和项目想把这个模型集成进去，但是这些产品很多只支持某些特定的生产环境比如Java，为了上一个机器学习模型去大动干戈修改环境配置很不划算，此时我们就可以考虑用预测模型标    
用tensorflow学习贝叶斯个性化排序(BPR)	2019年2月15日	21527	46	5	        在贝叶斯个性化排序(BPR)算法小结中，我们对贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)的原理做了讨论，本文我们将从实践的角度来使用BPR做一个简单的推荐。由于现有主流开源类库都没有BPR，同时它又比较简单，因此用tensorflow自己实现一个    
贝叶斯个性化排序(BPR)算法小结	2019年2月15日	48510	68	16	        在矩阵分解在协同过滤推荐算法中的应用中，我们讨论过像funkSVD之类的矩阵分解方法如何用于推荐。今天我们讲另一种在实际产品中用的比较多的推荐算法:贝叶斯个性化排序(Bayesian Personalized Ranking, 以下简称BPR)，它也用到了矩阵分解，但是和funkSVD家族却有很多不    
特征工程之特征预处理	2019年2月15日	31736	113	27	        在前面我们分别讨论了特征工程中的特征选择与特征表达，本文我们来讨论特征预处理的相关问题。主要包括特征的归一化和标准化，异常特征样本清洗与样本数据不平衡问题的处理。 1. 特征的标准化和归一化 由于标准化和归一化这两个词经常混用，所以本文不再区别标准化和归一化，而通过具体的标准化和归一化方法来区别具体    
特征工程之特征表达	2019年2月15日	28365	107	12	        在特征工程之特征选择中，我们讲到了特征选择的一些要点。本篇我们继续讨论特征工程，不过会重点关注于特征表达部分，即如果对某一个特征的具体表现形式做处理。主要包括缺失值处理，特殊的特征处理比如时间和地理位置处理，离散特征的连续化和离散化处理，连续特征的离散化处理几个方面。 1. 缺失值处理 特征有缺失值    
特征工程之特征选择	2019年2月15日	57438	115	31	        特征工程是数据分析中最耗时间和精力的一部分工作，它不像算法和模型那样是确定的步骤，更多是工程上的经验和权衡。因此没有统一的方法。这里只是对一些常用的方法做一个总结。本文关注于特征选择部分。后面还有两篇会关注于特征表达和特征预处理。 1. 特征的来源 在做数据分析的时候，特征的来源一般有两块，一块是业    
用gensim学习word2vec	2019年2月15日	95612	104	24	        在word2vec原理篇中，我们对word2vec的两种模型CBOW和Skip-Gram，以及两种解法Hierarchical Softmax和Negative Sampling做了总结。这里我们就从实践的角度，使用gensim来学习word2vec。 1. gensim安装与概述 gensim是一    
word2vec原理(三) 基于Negative Sampling的模型	2019年2月15日	106072	131	27	        word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 在上一篇中我们讲到了基于Hierarchical Softmax的word2ve    
word2vec原理(二) 基于Hierarchical Softmax的模型	2019年2月15日	137971	290	45	        word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 在word2vec原理(一) CBOW与Skip-Gram模型基础中，我们讲到了    
word2vec原理(一) CBOW与Skip-Gram模型基础	2019年2月15日	254498	112	44	        word2vec原理(一) CBOW与Skip-Gram模型基础 word2vec原理(二) 基于Hierarchical Softmax的模型 word2vec原理(三) 基于Negative Sampling的模型 word2vec是google在2013年推出的一个NLP工具，它的特点是将所有    
条件随机场CRF(三) 模型学习与维特比算法解码	2019年2月15日	28683	66	7	        条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三)&#160;模型学习与维特比算法解码 在CRF系列的前两篇，我们总结了CRF的模型基础与第一个问题的求解方法，本文我们关注于linear-CRF的第二个问题与第三个问题的求解。第二个问    
条件随机场CRF(二) 前向后向算法评估标记序列概率	2019年2月15日	23250	80	5	        条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 在条件随机场CRF(一)中我们总结了CRF的模型，主要是linear-CRF的模型原理。本文就继续讨论linear-CRF需要解决的三个问题：评估    
条件随机场CRF(一)从随机场到线性链条件随机场	2019年2月15日	55032	99	21	        条件随机场CRF(一)从随机场到线性链条件随机场 条件随机场CRF(二) 前向后向算法评估标记序列概率 条件随机场CRF(三) 模型学习与维特比算法解码 条件随机场(Conditional Random Fields, 以下简称CRF)是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然    
用hmmlearn学习隐马尔科夫模型HMM	2019年2月15日	55092	190	15	        在之前的HMM系列中，我们对隐马尔科夫模型HMM的原理以及三个问题的求解方法做了总结。本文我们就从实践的角度用Python的hmmlearn库来学习HMM的使用。关于hmmlearn的更多资料在官方文档有介绍。 1.&#160;hmmlearn概述 hmmlearn安装很简单，&quot;pip install hmm    
隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列	2019年2月15日	43487	35	8	        隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在本篇我们会讨论HMM模型最后一个问题的求解，即即给定模型和观测序列，求给定观测序列条件下，最    
隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数	2019年2月15日	40579	97	12	        隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在本篇我们会讨论HMM模型参数求解的问题，这个问题在HMM三个问题里算是最复杂的。在研究这个问    
隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率	2019年2月15日	63331	61	20	        隐马尔科夫模型HMM（一）HMM模型 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 在隐马尔科夫模型HMM（一）HMM模型中，我们讲到了HMM模型的基础知识和HMM的三个基本问题    
隐马尔科夫模型HMM（一）HMM模型	2019年2月15日	120036	37	35	        隐马尔科夫模型HMM（一）HMM模型基础 隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率 隐马尔科夫模型HMM（三）鲍姆-韦尔奇算法求解HMM参数 隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列 隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学    
EM算法原理总结	2019年2月15日	103761	131	33	        EM算法也称期望最大化（Expectation-Maximum,简称EM）算法，它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。本文就对EM算法的原理做一个总结。 1.&#160;EM算法要解决的问题 我们经常会从样本观察数据中，找出样本的模型参    
用scikit-learn学习LDA主题模型	2019年2月15日	67856	115	13	        在LDA模型原理篇我们总结了LDA主题模型的原理，这里我们就从应用的角度来使用scikit-learn来学习LDA主题模型。除了scikit-learn, 还有spark MLlib和gensim库也有LDA主题模型的类库，使用的原理基本类似，本文关注于scikit-learn中LDA主题模型的使用    
文本主题模型之LDA(三) LDA求解之变分推断EM算法	2019年2月15日	32855	101	11	        文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 本文是LDA主题模型的第三篇，读这一篇之前建议先读文本主题模型之LDA(一) LDA基础，同时由于使用了EM算法，如果你对EM算法不熟悉，建议    
文本主题模型之LDA(二) LDA求解之Gibbs采样算法	2019年2月15日	55410	218	10	        文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 本文是LDA主题模型的第二篇，读这一篇之前建议先读文本主题模型之LDA(一) LDA基础，同时由于使用了基于MCMC的Gibbs采样算法，如果    
文本主题模型之LDA(一) LDA基础	2019年2月15日	214245	105	29	        文本主题模型之LDA(一) LDA基础 文本主题模型之LDA(二) LDA求解之Gibbs采样算法 文本主题模型之LDA(三) LDA求解之变分推断EM算法 在前面我们讲到了基于矩阵分解的LSI和NMF主题模型，这里我们开始讨论被广泛使用的主题模型：隐含狄利克雷分布(Latent Dirichlet    
文本主题模型之非负矩阵分解(NMF)	2019年2月15日	31403	15	9	        在文本主题模型之潜在语义索引(LSI)中，我们讲到LSI主题模型使用了奇异值分解，面临着高维度计算量太大的问题。这里我们就介绍另一种基于矩阵分解的主题模型：非负矩阵分解(NMF)，它同样使用了矩阵分解，但是计算量和处理速度则比LSI快，它是怎么做到的呢？ 1.&#160;非负矩阵分解(NMF)概述 非负矩阵分    
文本主题模型之潜在语义索引(LSI)	2019年2月15日	40598	48	11	        在文本挖掘中，主题模型是比较特殊的一块，它的思想不同于我们常用的机器学习算法，因此这里我们需要专门来总结文本主题模型的算法。本文关注于潜在语义索引算法(LSI)的原理。 1. 文本主题模型的问题特点 在数据分析中，我们经常会进行非监督学习的聚类算法，它可以对我们的特征数据进行非监督的聚类。而主题模型    
英文文本挖掘预处理流程总结	2019年2月15日	27722	16	9	        在中文文本挖掘预处理流程总结中，我们总结了中文文本挖掘的预处理流程，这里我们再对英文文本挖掘的预处理流程做一个总结。 1. 英文文本挖掘预处理特点 英文文本的预处理方法和中文的有部分区别。首先，英文文本挖掘预处理一般可以不做分词（特殊需求除外），而中文预处理分词是必不可少的一步。第二点，大部分英文文    
中文文本挖掘预处理流程总结	2019年2月15日	59172	76	20	        在对文本做数据分析时，我们一大半的时间都会花在文本预处理上，而中文和英文的预处理流程稍有不同，本文就对中文文本挖掘的预处理流程做一个总结。 1. 中文文本挖掘预处理特点 首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。 首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接    
文本挖掘预处理之TF-IDF	2019年2月15日	77757	39	20	        在文本挖掘预处理之向量化与Hash Trick中我们讲到在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。 1. 文本向量化特征的不足 在将文本分词并向量化后，我们可以得到词汇表中每个词在各    
文本挖掘预处理之向量化与Hash Trick	2019年2月15日	28280	36	8	        在文本挖掘的分词原理中，我们讲到了文本挖掘的预处理的关键一步：“分词”，而在做了分词后，如果我们是做文本分类聚类，则后面关键的特征预处理步骤有向量化或向量化的特例Hash Trick，本文我们就对向量化和特例Hash Trick预处理方法做一个总结。 1. 词袋模型 在讲向量化与Hash Trick    
文本挖掘的分词原理	2019年2月15日	37997	65	17	        在做文本挖掘的时候，首先要做的预处理就是分词。英文单词天然有空格隔开容易按照空格分词，但是也有时候需要把多个单词做为一个分词，比如一些名词如“New York”，需要做为一个词看待。而中文由于没有空格，分词就是一个需要专门去解决的问题了。无论是英文还是中文，分词的原理都是类似的，本文就对文本挖掘时的    
MCMC(四)Gibbs采样	2019年2月15日	102216	145	30	        MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(三)MCMC采样和M-H采样中，我们讲到了M-H采样已经可以很好的解决蒙特卡罗方法需要的任意概率分布的样本集的问题。但是M-H采样有两个缺点：一是需要计算接受率，在    
MCMC(三)MCMC采样和M-H采样	2019年2月15日	122735	267	34	        MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(二)马尔科夫链中我们讲到给定一个概率平稳分布$\pi$, 很难直接找到对应的马尔科夫链状态转移矩阵$P$。而只要解决这个问题，我们就可以找到一种通用的概率分布采样方    
MCMC(二)马尔科夫链	2019年2月15日	120969	150	49	        MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 在MCMC(一)蒙特卡罗方法中，我们讲到了如何用蒙特卡罗方法来随机模拟求解一些复杂的连续积分或者离散求和的方法，但是这个方法需要得到对应的概率分布的样本集，而想得到这样的样本集    
MCMC(一)蒙特卡罗方法	2019年2月15日	182371	116	69	        MCMC(一)蒙特卡罗方法 MCMC(二)马尔科夫链 MCMC(三)MCMC采样和M-H采样 MCMC(四)Gibbs采样 作为一种随机采样方法，马尔科夫链蒙特卡罗（Markov Chain Monte Carlo，以下简称MCMC）在机器学习,深度学习以及自然语言处理等领域都有广泛的应用，是很多复    
受限玻尔兹曼机（RBM）原理总结	2019年2月15日	48668	44	17	        在前面我们讲到了深度学习的两类神经网络模型的原理，第一类是前向的神经网络，即DNN和CNN。第二类是有反馈的神经网络，即RNN和LSTM。今天我们就总结下深度学习里的第三类神经网络模型：玻尔兹曼机。主要关注于这类模型中的受限玻尔兹曼机（Restricted Boltzmann Machine，以下简    
LSTM模型与前向反向传播算法	2019年2月15日	95387	173	34	        在循环神经网络(RNN)模型与前向反向传播算法中，我们总结了对RNN模型做了总结。由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。    
循环神经网络(RNN)模型与前向反向传播算法	2019年2月15日	156406	217	28	        在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。今天我们就讨论另一类输出和模型间有反馈的神经网络：循环神经网络(Recurrent Neural Networks ，以下简称RNN)，它广泛的用于自然语言处理中的语音识    
卷积神经网络(CNN)反向传播算法	2019年2月15日	204700	256	61	        在卷积神经网络(CNN)前向传播算法中，我们对CNN的前向传播算法做了总结，基于CNN前向传播算法的基础，我们下面就对CNN的反向传播算法做一个总结。在阅读本文前，建议先研究DNN的反向传播算法：深度神经网络（DNN）反向传播算法(BP) 1. 回顾DNN的反向传播算法 我们首先回顾DNN的反向传播    
卷积神经网络(CNN)前向传播算法	2019年2月15日	70604	56	17	        在卷积神经网络(CNN)模型结构中，我们对CNN的模型结构做了总结，这里我们就在CNN的模型基础上，看看CNN的前向传播算法是什么样子的。重点会和传统的DNN比较讨论。 1. 回顾CNN的结构 在上一篇里，我们已经讲到了CNN的结构，包括输出层，若干的卷积层+ReLU激活函数，若干的池化层，DNN全    
卷积神经网络(CNN)模型结构	2019年2月15日	212000	79	32	        在前面我们讲述了DNN的模型与前向反向传播算法。而在DNN大类中，卷积神经网络(Convolutional Neural Networks，以下简称CNN)是最为成功的DNN特例之一。CNN广泛的应用于图像识别，当然现在也应用于NLP等其他领域，本文我们就对CNN的模型结构做一个总结。 在学习CNN    
深度神经网络（DNN）的正则化	2019年2月15日	41052	40	13	        和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，这里我们就对DNN的正则化方法做一个总结。 1. DNN的L1&amp;L2正则化 想到正则化，我们首先想到的就是L1正则化和L2正则化。L1正则化和L2正则化原理类似，这里重点讲述DNN的L2正则化。 而DNN的L2正则化通常的做法是只针    
深度神经网络（DNN）损失函数和激活函数的选择	2019年2月15日	87295	164	15	        在深度神经网络（DNN）反向传播算法(BP)中，我们对DNN的前向反向传播算法的使用做了总结。里面使用的损失函数是均方差，而激活函数是Sigmoid。实际上DNN可以使用的损失函数和激活函数不少。这些损失函数和激活函数如何选择呢？下面我们就对DNN损失函数和激活函数的选择做一个总结。 1. 均方差损    
深度神经网络（DNN）反向传播算法(BP)	2019年2月15日	133178	176	40	        在深度神经网络（DNN）模型与前向传播算法中，我们对DNN的模型和前向传播算法做了总结，这里我们更进一步，对DNN的反向传播算法（Back Propagation，BP）做一个总结。 1. DNN反向传播算法要解决的问题 在了解DNN的反向传播算法前，我们先要知道DNN反向传播算法要解决的问题，也就    
深度神经网络（DNN）模型与前向传播算法	2019年2月15日	222842	47	52	        深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。 1. 从感知机到神经网络 在感知机原理小结中，我们介绍过感知机的模型，它是一个有若干输入和一个输出的模型，如下图:    
分解机(Factorization Machines)推荐算法原理	2019年2月15日	52094	72	11	        对于分解机(Factorization Machines，FM)推荐算法原理，本来想自己单独写一篇的。但是看到peghoty写的FM不光简单易懂，而且排版也非常好，因此转载过来，自己就不再单独写FM了。 Pinard注：上面最后一句话应该是&quot;而$g_{\theta}(x)$则利用$\widehat{    
用Spark学习矩阵分解推荐算法	2019年2月15日	21933	44	6	        在矩阵分解在协同过滤推荐算法中的应用中，我们对矩阵分解在推荐算法中的应用原理做了总结，这里我们就从实践的角度来用Spark学习矩阵分解推荐算法。 1. Spark推荐算法概述 在Spark MLlib中，推荐算法这块只实现了基于矩阵分解的协同过滤推荐算法。而基于的算法是FunkSVD算法，即将m个用    
SimRank协同过滤推荐算法	2019年2月15日	20002	36	5	        在协同过滤推荐算法总结中，我们讲到了用图模型做协同过滤的方法，包括SimRank系列算法和马尔科夫链系列算法。现在我们就对SimRank算法在推荐系统的应用做一个总结。 1.&#160;SimRank推荐算法的图论基础 SimRank是基于图论的，如果用于推荐算法，则它假设用户和物品在空间中形成了一张图。而这    
矩阵分解在协同过滤推荐算法中的应用	2019年2月15日	48157	86	19	        在协同过滤推荐算法总结中，我们讲到了用矩阵分解做协同过滤是广泛使用的方法，这里就对矩阵分解在协同过滤推荐算法中的应用做一个总结。(过年前最后一篇！祝大家新年快乐！明年的目标是写120篇机器学习，深度学习和NLP相关的文章) 1.&#160;矩阵分解用于推荐算法要解决的问题 在推荐系统中，我们常常遇到的问题是这    
协同过滤推荐算法总结	2019年2月15日	95539	66	41	        推荐算法具有非常多的应用场景和商业价值，因此对推荐算法值得好好研究。推荐算法种类很多，但是目前应用最广泛的应该是协同过滤类别的推荐算法，本文就对协同过滤类别的推荐算法做一个概括总结，后续也会对一些典型的协同过滤推荐算法做原理总结。 1. 推荐算法概述 推荐算法是非常古老的，在机器学习还没有兴起的时候    
用Spark学习FP Tree算法和PrefixSpan算法	2019年2月15日	14291	24	4	        在FP Tree算法原理总结和PrefixSpan算法原理总结中，我们对FP Tree和PrefixSpan这两种关联算法的原理做了总结，这里就从实践的角度介绍如何使用这两个算法。由于scikit-learn中没有关联算法的类库，而Spark MLlib有，本文的使用以Spark MLlib作为使用    
PrefixSpan算法原理总结	2019年2月15日	39794	32	8	        前面我们讲到频繁项集挖掘的关联算法Apriori和FP Tree。这两个算法都是挖掘频繁项集的。而今天我们要介绍的PrefixSpan算法也是关联算法，但是它是挖掘频繁序列模式的，因此要解决的问题目标稍有不同。 1.&#160;项集数据和序列数据 首先我们看看项集数据和序列数据有什么不同，如下图所示。 左边的    
FP Tree算法原理总结	2019年2月15日	84920	80	48	        在Apriori算法原理总结中，我们对Apriori算法的原理做了总结。作为一个挖掘频繁项集的算法，Apriori算法需要多次扫描数据，I/O是很大的瓶颈。为了解决这个问题，FP Tree算法（也称FP Growth算法）采用了一些技巧，无论多少数据，只需要扫描两次数据集，因此提高了算法运行的效率。    
Apriori算法原理总结	2019年2月15日	123925	52	31	        Apriori算法是常用的用于挖掘出数据关联规则的算法，它用来找出数据值中频繁出现的数据集合，找出这些集合的模式有助于我们做一些决策。比如在常见的超市购物数据集，或者电商的网购数据集中，如果我们找到了频繁出现的数据集，那么对于超市，我们可以优化产品的位置摆放，对于电商，我们可以优化商品所在的仓库位置    
典型关联分析(CCA)原理总结	2019年2月15日	70378	59	20	        典型关联分析(Canonical Correlation Analysis，以下简称CCA)是最常用的挖掘数据关联关系的算法之一。比如我们拿到两组数据，第一组是人身高和体重的数据，第二组是对应的跑步能力和跳远能力的数据。那么我们能不能说这两组数据是相关的呢？CCA可以帮助我们分析这个问题。 1. C    
用scikit-learn研究局部线性嵌入(LLE)	2019年2月15日	11599	0	5	        在局部线性嵌入(LLE)原理总结中，我们对流形学习中的局部线性嵌入(LLE)算法做了原理总结。这里我们就对scikit-learn中流形学习的一些算法做一个介绍，并着重对其中LLE算法的使用方法做一个实践上的总结。 1.&#160;scikit-learn流形学习库概述 在scikit-learn中，流形学习    
局部线性嵌入(LLE)原理总结	2019年2月15日	68327	94	15	        局部线性嵌入(Locally Linear Embedding，以下简称LLE)也是非常重要的降维方法。和传统的PCA，LDA等关注样本方差的降维方法相比，LLE关注于降维时保持样本局部的线性特征，由于LLE在降维时保持了样本的局部特征，它广泛的用于图像图像识别，高维数据可视化等领域。下面我们就对L    
奇异值分解(SVD)原理与在降维中的应用	2019年2月15日	291471	125	124	        奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SV    
用scikit-learn进行LDA降维	2019年2月15日	42552	29	8	        在线性判别分析LDA原理总结中，我们对LDA降维的原理做了总结，这里我们就对scikit-learn中LDA的降维使用做一个总结。 1.&#160;对scikit-learn中LDA类概述 在scikit-learn中， LDA类是sklearn.discriminant_analysis.LinearDis    
线性判别分析LDA原理总结	2019年2月15日	301137	207	57	        在主成分分析（PCA）原理总结中，我们对降维算法PCA做了总结。这里我们就对另外一种经典的降维方法线性判别分析（Linear Discriminant Analysis, 以下简称LDA）做一个总结。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了    
用scikit-learn学习主成分分析(PCA)	2019年2月15日	157638	74	18	        在主成分分析（PCA）原理总结中，我们对主成分分析(以下简称PCA)的原理做了总结，下面我们就总结下如何使用scikit-learn工具来进行PCA降维。 1. scikit-learn PCA类介绍 在scikit-learn中，与PCA相关的类都在sklearn.decomposition包中。    
主成分分析（PCA）原理总结	2019年2月15日	238963	257	68	        主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。一般我们提到降维最容易想到的算法就是PCA，下面我们就对PCA的原理做一个总结。 1. PCA的思想 PCA顾名思义，就是找出数据里最    
用scikit-learn学习谱聚类	2019年2月15日	42765	47	8	        在谱聚类（spectral clustering）原理总结中，我们对谱聚类的原理做了总结。这里我们就对scikit-learn中谱聚类的使用做一个总结。 1. scikit-learn谱聚类概述 在scikit-learn的类库中，sklearn.cluster.SpectralClustering    
谱聚类（spectral clustering）原理总结	2019年2月15日	334131	299	88	        谱聚类（spectral clustering）是广泛使用的聚类算法，比起传统的K-Means算法，谱聚类对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多，更加难能可贵的是实现起来也不复杂。在处理实际的聚类问题时，个人认为谱聚类是应该首先考虑的几种算法之一。下面我们就对谱聚类的算法    
用scikit-learn学习DBSCAN聚类	2019年2月15日	129251	78	14	        在DBSCAN密度聚类算法中，我们对DBSCAN聚类算法的原理做了总结，本文就对如何用scikit-learn来学习DBSCAN聚类做一个总结，重点讲述参数的意义和需要调参的参数。 1. scikit-learn中的DBSCAN类 在scikit-learn中，DBSCAN算法类为sklearn.c    
DBSCAN密度聚类算法	2019年2月15日	258316	80	38	        DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用    
用scikit-learn学习BIRCH聚类	2019年2月15日	26142	55	5	        在BIRCH聚类算法原理中，我们对BIRCH聚类算法的原理做了总结，本文就对scikit-learn中BIRCH算法的使用做一个总结。 1. scikit-learn之BIRCH类 在scikit-learn中，BIRCH类实现了原理篇里讲到的基于特征树CF Tree的聚类。因此要使用BIRCH来聚    
BIRCH聚类算法原理	2019年2月15日	76741	74	25	        在K-Means聚类算法原理中，我们讲到了K-Means和Mini Batch K-Means的聚类原理。这里我们再来看看另外一种常见的聚类算法BIRCH。BIRCH算法比较适合于数据量大，类别数K也比较多的情况。它运行速度很快，只需要单遍扫描数据集就能进行聚类，当然需要用到一些技巧，下面我们就对B    
用scikit-learn学习K-Means聚类	2019年2月15日	119953	73	17	        在K-Means聚类算法原理中，我们对K-Means的原理做了总结，本文我们就来讨论用scikit-learn来学习K-Means聚类。重点讲述如何选择合适的k值。 1. K-Means类概述 在scikit-learn中，包括两个K-Means的算法，一个是传统的K-Means算法，对应的类是KM    
K-Means聚类算法原理	2019年2月15日	334815	79	41	        K-Means算法是无监督的聚类算法，它实现起来比较简单，聚类效果也不错，因此应用很广泛。K-Means算法有大量的变体，本文就从最传统的K-Means算法讲起，在其基础上讲述K-Means的优化变体方法。包括初始化优化K-Means++, 距离计算优化elkan K-Means算法和大数据情况下的    
scikit-learn随机森林调参小结	2019年2月15日	165920	166	34	        在Bagging与随机森林算法原理小结中，我们对随机森林(Random Forest, 以下简称RF）的原理做了总结。本文就从实践的角度对RF做一个总结。重点讲述scikit-learn中RF的调参注意事项，以及和GBDT调参的异同点。 1. scikit-learn随机森林类库概述 在scikit    
Bagging与随机森林算法原理小结	2019年2月15日	138372	165	35	        在集成学习原理小结中，我们讲到了集成学习有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。本文就对集成学习中Bagging与随机森林算法做一个总结。 随机森林是集成学习中可以和梯度提升树GB    
scikit-learn 梯度提升树(GBDT)调参小结	2019年2月15日	132922	108	26	        在梯度提升树(GBDT)原理小结中，我们对GBDT的原理做了总结，本文我们就从scikit-learn里GBDT的类库使用方法作一个总结，主要会关注调参中的一些要点。 1.&#160;scikit-learn GBDT类库概述 在sacikit-learn中，GradientBoostingClassifie    
梯度提升树(GBDT)原理小结	2019年2月15日	340331	617	67	        在集成学习之Adaboost算法原理小结中，我们对Boosting家族的Adaboost算法做了总结，本文就对Boosting家族中另一个重要的算法梯度提升树(Gradient Boosting Decison Tree, 以下简称GBDT)做一个总结。GBDT有很多简称，有GBT（Gradient    
scikit-learn Adaboost类库使用小结	2019年2月15日	100244	118	31	        在集成学习之Adaboost算法原理小结中，我们对Adaboost的算法原理做了一个总结。这里我们就从实用的角度对scikit-learn中Adaboost类库的使用做一个小结，重点对调参的注意事项做一个总结。 1. Adaboost类库概述 scikit-learn中Adaboost类库比较直接，    
集成学习之Adaboost算法原理小结	2019年2月15日	169614	362	70	        在集成学习原理小结中，我们讲到了集成学习按照个体学习器之间是否存在依赖关系可以分为两类，第一个是个体学习器之间存在强依赖关系，另一类是个体学习器之间不存在强依赖关系。前者的代表算法就是是boosting系列算法。在boosting系列算法中， Adaboost是最著名的算法之一。Adaboost既可    
集成学习原理小结	2019年2月15日	103103	55	67	        集成学习(ensemble learning)可以说是现在非常火爆的机器学习方法了。它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。也就是我们常说的“博采众长”。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等等，可以说所有的机器学习领域都    
支持向量机高斯核调参小结	2019年2月15日	42475	60	18	        在支持向量机(以下简称SVM)的核函数中，高斯核(以下简称RBF)是最常用的，从理论上讲， RBF一定不比线性核函数差，但是在实际应用中，却面临着几个重要的超参数的调优问题。如果调的不好，可能比线性核函数还要差。所以我们实际应用中，能用线性核函数得到较好效果的都会选择线性核函数。如果线性核不好，我们    
scikit-learn 支持向量机算法库使用小结	2019年2月15日	47873	40	12	        之前通过一个系列对支持向量机(以下简称SVM)算法的原理做了一个总结，本文从实践的角度对scikit-learn SVM算法库的使用做一个小结。scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分。 1.&#160;scikit-learn SVM    
支持向量机原理(五)线性支持回归	2019年2月15日	43065	41	11	        支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在前四篇里面我们讲到了SVM的线性分类和非线性分类，以及在分类时用到的算法。这些都关注    
支持向量机原理(三)线性不可分支持向量机与核函数	2019年2月15日	37387	43	14	        支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在前面两篇我们讲到了线性可分SVM的硬间隔最大化和软间隔最大化的算法，它们对线性可分的    
支持向量机原理(二) 线性支持向量机的软间隔最大化模型	2019年2月15日	44738	73	20	        支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 在支持向量机原理(一) 线性支持向量机中，我们对线性可分SVM的模型和损失函数优化做了    
支持向量机原理(一) 线性支持向量机	2019年2月15日	118462	124	47	        支持向量机原理(一) 线性支持向量机 支持向量机原理(二) 线性支持向量机的软间隔最大化模型 支持向量机原理(三)线性不可分支持向量机与核函数 支持向量机原理(四)SMO算法原理 支持向量机原理(五)线性支持回归 支持向量机(Support Vecor Machine,以下简称SVM)虽然诞生只有短    
最大熵模型原理小结	2019年2月15日	52743	100	15	        最大熵模型(maximum entropy model， MaxEnt)也是很典型的分类算法了，它和逻辑回归类似，都是属于对数线性分类模型。在损失函数优化的过程中，使用了和支持向量机类似的凸优化技术。而对熵的使用，让我们想起了决策树算法中的ID3和C4.5算法。理解了最大熵模型，对逻辑回归，支持向量    
scikit-learn 朴素贝叶斯类库使用小结	2019年2月15日	57597	42	22	        之前在朴素贝叶斯算法原理小结这篇文章中，对朴素贝叶斯分类算法的原理做了一个总结。这里我们就从实战的角度来看朴素贝叶斯类库。重点讲述scikit-learn 朴素贝叶斯类库的使用要点和参数选择。 1. scikit-learn 朴素贝叶斯类库概述 朴素贝叶斯是一类比较简单的算法，scikit-lear    
朴素贝叶斯算法原理小结	2019年2月15日	131611	130	48	        在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数$Y=f(X)$,要么是条件分布$P(Y|X)$。但是朴素贝叶斯却是生成方法，也就是直    
scikit-learn K近邻法类库使用小结	2019年2月15日	39086	13	10	        在K近邻法(KNN)原理小结这篇文章，我们讨论了KNN的原理和优缺点，这里我们就从实践出发，对scikit-learn 中KNN相关的类库使用做一个小结。主要关注于类库调参时的一个经验总结。 1.&#160;scikit-learn 中KNN相关的类库概述 在scikit-learn 中，与近邻法这一大类相关    
K近邻法(KNN)原理小结	2019年2月15日	87949	87	28	        K近邻法(k-nearest neighbors,KNN)是一种很基本的机器学习方法了，在我们平常的生活中也会不自主的应用。比如，我们判断一个人的人品，只需要观察他来往最密切的几个人的人品好坏就可以得出了。这里就运用了KNN的思想。KNN方法既可以做分类，也可以做回归，这点和决策树算法相同。 KNN    
scikit-learn决策树算法类库使用小结	2019年2月15日	149409	137	30	        之前对决策树的算法原理做了总结，包括决策树算法原理(上)和决策树算法原理(下)。今天就从实践的角度来介绍决策树算法，主要是讲解使用scikit-learn来跑决策树算法，结果的可视化以及一些参数调参的关键点。 1.&#160;scikit-learn决策树算法类库介绍 scikit-learn决策树算法类库内    
决策树算法原理(下)	2019年2月15日	130636	342	52	        在决策树算法原理(上)这篇里，我们讲到了决策树里ID3算法，和ID3算法的改进版C4.5算法。对于C4.5算法，我们也提到了它的不足，比如模型是用较为复杂的熵来度量，使用了相对较为复杂的多叉树，只能处理分类不能处理回归等。对于这些问题， CART算法大部分做了改进。CART算法也就是我们下面的重点了    
决策树算法原理(上)	2019年2月15日	136562	144	50	        决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林。本文就对决策树算法原理做一个总结，上篇对ID3， C4.5的算法思想做了总结，下篇重点对CART算法做一个详细的介绍。选择CART做重点介绍的原因是scikit-learn    
机器学习算法的随机数据生成	2019年2月15日	35570	11	20	        在学习机器学习算法的过程中，我们经常需要数据来验证算法，调试参数。但是找到一组十分合适某种特定算法类型的数据样本却不那么容易。还好numpy, scikit-learn都提供了随机数据生成的功能，我们可以自己生成适合某一种模型的数据，用随机数据来做清洗，归一化，转换，然后选择模型与算法做拟合和预测。    
感知机原理小结	2019年2月15日	59596	115	41	        感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。这里对感知机的原理做一个小结。     
日志和告警数据挖掘经验谈	2019年2月15日	18545	17	5	        最近参与了了一个日志和告警的数据挖掘项目，里面用到的一些思路在这里和大家做一个分享。 项目的需求是收集的客户系统一个月300G左右的的日志和告警数据做一个整理，主要是归类(Grouping)和关联(Correlation)，从而得到告警和日志的一些统计关系，这些统计结果可以给一线支持人员参考。 得到    
scikit-learn 逻辑回归类库使用小结	2019年2月15日	51766	66	17	        之前在逻辑回归原理小结这篇文章中，对逻辑回归的原理做了小结。这里接着对scikit-learn中逻辑回归类库的我的使用经验做一个总结。重点讲述调参中要注意的事项。 1. 概述 在scikit-learn中，与逻辑回归有关的主要是这3个类。LogisticRegression， LogisticReg    
逻辑回归原理小结	2019年2月15日	109615	199	41	        逻辑回归是一个分类算法，它可以处理二元分类以及多元分类。虽然它名字里面有“回归”两个字，却不是一个回归算法。那为什么有“回归”这个误导性的词呢？个人认为，虽然逻辑回归是分类模型，但是它的原理里面却残留着回归模型的影子，本文对逻辑回归原理做一个总结。 1. 从线性回归到逻辑回归 我们知道，线性回归的模    
scikit-learn 线性回归算法库小结	2019年2月15日	36470	27	13	        scikit-learn对于线性回归提供了比较多的类库，这些类库都可以用来做线性回归分析，本文就对这些类库的使用做一个总结，重点讲述这些线性回归算法库的不同和各自的使用场景。 线性回归的目的是要得到输出向量\(\mathbf{Y}\)和输入特征\(\mathbf{X}\)之间的线性关系，求出线性回归    
用scikit-learn和pandas学习Ridge回归	2019年2月15日	30800	44	11	        本文将用一个例子来讲述怎么用scikit-learn和pandas来学习Ridge回归。 1. Ridge回归的损失函数 在我的另外一遍讲线性回归的文章中，对Ridge回归做了一些介绍，以及什么时候适合用 Ridge回归。如果对什么是Ridge回归还完全不清楚的建议阅读我这篇文章。 线性回归原理小结    
Lasso回归算法： 坐标轴下降法与最小角回归法小结	2019年2月15日	71729	73	24	        前面的文章对线性回归做了一个小结，文章在这：&#160;线性回归原理小结。里面对线程回归的正则化也做了一个初步的介绍。提到了线程回归的L2正则化-Ridge回归，以及线程回归的L1正则化-Lasso回归。但是对于Lasso回归的解法没有提及，本文是对该文的补充和扩展。以下都用矩阵法表示，如果对于矩阵分析不熟悉    
用scikit-learn和pandas学习线性回归	2019年2月15日	105266	77	21	        对于想深入了解线性回归的童鞋，这里给出一个完整的例子，详细学完这个例子，对用scikit-learn来运行线性回归，评估模型不会有什么问题了。 1. 获取数据，定义问题 没有数据，当然没法研究机器学习啦。:) 这里我们用UCI大学公开的机器学习数据来跑线性回归。 数据的介绍在这：&#160;http://ar    
scikit-learn 和pandas 基于windows单机机器学习环境的搭建	2019年2月15日	17691	8	4	        很多朋友想学习机器学习，却苦于环境的搭建，这里给出windows上scikit-learn研究开发环境的搭建步骤。 Step 1. Python的安装 python有2.x和3.x的版本之分，但是很多好的机器学习python库都不支持3.x，因此，推荐安装2.7版本的python。当前最新的pyth    
机器学习研究与开发平台的选择	2019年2月15日	24103	30	15	        目前机器学习可以说是百花齐放阶段，不过如果要学习或者研究机器学习，进而用到生产环境，对平台，开发语言，机器学习库的选择就要费一番脑筋了。这里就我自己的机器学习经验做一个建议，仅供参考。 首先，对于平台选择的第一个问题是，你是要用于生产环境，也就是具体的产品中,还是仅仅是做研究学习用？ 1. 生产环境    
线性回归原理小结	2019年2月15日	60826	111	24	        线性回归可以说是机器学习中最基本的问题类型了，这里就对线性回归的原理和算法做一个小结。 1. 线性回归的模型函数和损失函数 线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应于n维特征和一个结果输出，如下： \((x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_    
精确率与召回率，RoC曲线与PR曲线	2019年2月15日	42227	35	16	        在机器学习的算法评估中，尤其是分类算法评估中，我们经常听到精确率(precision)与召回率(recall)，RoC曲线与PR曲线这些概念，那这些概念到底有什么用处呢？ 首先，我们需要搞清楚几个拗口的概念： 1. TP, FP, TN, FN 听起来还是很费劲，不过我们用一张图就很容易理解了。图如    
最小二乘法小结	2019年2月15日	108660	72	58	        最小二乘法是用来做函数拟合或者求函数极值的方法。在机器学习，尤其是回归模型中，经常可以看到最小二乘法的身影，这里就对我对最小二乘法的认知做一个小结。 1.最小二乘法的原理与要解决的问题 最小二乘法是由勒让德在19世纪发现的，原理的一般形式很简单，当然发现的过程是非常艰难的。形式如下式：$$目标函数     
梯度下降（Gradient Descent）小结	2019年2月15日	544352	247	166	        在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。这里就对梯度下降法做一个完整的总结。 1. 梯度 在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数    
